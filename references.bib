
@book{hogan_knowledge_2022,
	address = {Cham},
	series = {Synthesis {Lectures} on {Data}, {Semantics}, and {Knowledge}},
	title = {Knowledge {Graphs}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-00790-3 978-3-031-01918-0},
	url = {https://link.springer.com/10.1007/978-3-031-01918-0},
	language = {en},
	urldate = {2025-04-17},
	publisher = {Springer International Publishing},
	author = {Hogan, Aidan and Gutierrez, Claudio and Cochez, Michael and Melo, Gerard De and Kirrane, Sabrina and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Schmelzeisen, Lukas and Staab, Steffen and Blomqvist, Eva and d’Amato, Claudia and Gayo, José Emilio Labra and Neumaier, Sebastian and Rula, Anisa and Sequeda, Juan and Zimmermann, Antoine},
	year = {2022},
	doi = {10.1007/978-3-031-01918-0},
}

@article{pan_approximating_nodate,
	title = {Approximating {OWL}-{DL} {Ontologies}},
	abstract = {Efﬁcient query answering over ontologies is one of the most useful and important services to support Semantic Web applications. Approximation has been identiﬁed as a potential way to reduce the complexity of query answering over OWL DL ontologies. Existing approaches are mainly based on syntactic approximation of ontological axioms and queries. In this paper, we propose to recast the idea of knowledge compilation into approximating OWL DL ontologies with DL-Lite ontologies, against which query answering has only polynomial data complexity. We identify a useful category of queries for which our approach guarantees also completeness. Furthermore, this paper reports on the implementation of our approach in the ONTOSEARCH2 system and preliminary, but encouraging, benchmark results which compare ONTOSEARCH2’s response times on a number of queries with those of existing ontology reasoning systems.},
	language = {en},
	author = {Pan, Jeff Z},
}

@incollection{hutchison_owl_2013,
	address = {Berlin, Heidelberg},
	title = {From {OWL} to {DL} − {Lite} through {Efficient} {Ontology} {Approximation}},
	volume = {7994},
	isbn = {978-3-642-39665-6 978-3-642-39666-3},
	url = {http://link.springer.com/10.1007/978-3-642-39666-3_20},
	language = {en},
	urldate = {2025-10-01},
	booktitle = {Web {Reasoning} and {Rule} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Console, Marco and Santarelli, Valerio and Savo, Domenico Fabio},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Faber, Wolfgang and Lembo, Domenico},
	year = {2013},
	doi = {10.1007/978-3-642-39666-3_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {229--234},
}

@article{calvanese_tractable_2007,
	title = {Tractable {Reasoning} and {Efficient} {Query} {Answering} in {Description} {Logics}: {The} {DL}-{Lite} {Family}},
	volume = {39},
	issn = {1573-0670},
	shorttitle = {Tractable {Reasoning} and {Efficient} {Query} {Answering} in {Description} {Logics}},
	url = {https://doi.org/10.1007/s10817-007-9078-x},
	doi = {10.1007/s10817-007-9078-x},
	abstract = {We propose a new family of description logics (DLs), called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, unions of conjunctive queries) over the instance level (ABox) of the DL knowledge base. We show that, for the DLs of the DL-Lite family, the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is LogSpace in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial-time data complexity for query answering over DL knowledge bases. Notably our logics allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current database management systems. Since even slight extensions to the logics of the DL-Lite family make query answering at least NLogSpace in data complexity, thus ruling out the possibility of using on-the-shelf relational technology for query processing, we can conclude that the logics of the DL-Lite family are the maximal DLs supporting efficient query answering over large amounts of instances.},
	language = {en},
	number = {3},
	urldate = {2025-10-01},
	journal = {Journal of Automated Reasoning},
	author = {Calvanese, Diego and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Rosati, Riccardo},
	month = oct,
	year = {2007},
	keywords = {DL-Lite, Description logics, Ontology languages, Query answering},
	pages = {385--429},
}

@article{artale_dl-lite_2009,
	title = {The {DL}-{Lite} {Family} and {Relations}},
	volume = {36},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10623},
	doi = {10.1613/jair.2820},
	abstract = {The recently introduced series of description logics under the common moniker ‘DLLite’ has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent conceptual modeling formalisms, on the other. The main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original DL-Lite logics along ﬁve axes: by (i) adding the Boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reﬂexivity, irreﬂexivity and transitivity constraints, and (v) adopting or dropping the unique name assumption. We analyze the combined complexity of satisﬁability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries. Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable ﬁrst-order logic, which provides useful insights into their properties and, in particular, computational behavior.},
	language = {en},
	urldate = {2025-10-01},
	journal = {Journal of Artificial Intelligence Research},
	author = {Artale, A. and Calvanese, D. and Kontchakov, R. and Zakharyaschev, M.},
	month = oct,
	year = {2009},
	pages = {1--69},
}

@inproceedings{roussey_catalogue_2009,
	address = {Redondo Beach California USA},
	title = {A catalogue of {OWL} ontology antipatterns},
	isbn = {978-1-60558-658-8},
	url = {https://dl.acm.org/doi/10.1145/1597735.1597784},
	doi = {10.1145/1597735.1597784},
	abstract = {Debugging inconsistent OWL ontologies is a timeconsuming task. Debugging services included in existing ontology engineering tools are still far from providing adequate support to ontology developers and domain experts for this task, due to their lack of efficiency or precision when explaining the main causes for inconsistencies. We present a catalogue of common antipatterns found in inconsistent ontologies that can be used in combination with these tools to make this task more effective.},
	language = {en},
	urldate = {2025-10-01},
	booktitle = {Proceedings of the fifth international conference on {Knowledge} capture},
	publisher = {ACM},
	author = {Roussey, Catherine and Corcho, Oscar and Vilches-Blázquez, Luis Manuel},
	month = sep,
	year = {2009},
	pages = {205--206},
}

@inproceedings{de_groot_analysing_2021,
	address = {Cham},
	title = {Analysing {Large} {Inconsistent} {Knowledge} {Graphs} {Using} {Anti}-patterns},
	isbn = {978-3-030-77385-4},
	doi = {10.1007/978-3-030-77385-4_3},
	abstract = {A number of Knowledge Graphs (KGs) on the Web of Data contain contradicting statements, and therefore are logically inconsistent. This makes reasoning limited and the knowledge formally useless. Understanding how these contradictions are formed, how often they occur, and how they vary between different KGs is essential for fixing such contradictions, or developing better tools that handle inconsistent KGs. Methods exist to explain a single contradiction, by finding the minimal set of axioms sufficient to produce it, a process known as justification retrieval. In large KGs, these justifications can be frequent and might redundantly refer to the same type of modelling mistake. Furthermore, these justifications are –by definition– domain dependent, and hence difficult to interpret or compare. This paper uses the notion of anti-pattern for generalising these justifications, and presents an approach for detecting almost all anti-patterns from any inconsistent KG. Experiments on KGs of over 28 billion triples show the scalability of this approach, and the benefits of anti-patterns for analysing and comparing logical errors between different KGs.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer International Publishing},
	author = {de Groot, Thomas and Raad, Joe and Schlobach, Stefan},
	editor = {Verborgh, Ruben and Hose, Katja and Paulheim, Heiko and Champin, Pierre-Antoine and Maleshkova, Maria and Corcho, Oscar and Ristoski, Petar and Alam, Mehwish},
	year = {2021},
	keywords = {Inconsistency, Linked open data, Reasoning},
	pages = {40--56},
}

@misc{ott_safran_2021,
	title = {{SAFRAN}: {An} interpretable, rule-based link prediction method outperforming embedding models},
	shorttitle = {{SAFRAN}},
	url = {http://arxiv.org/abs/2109.08002},
	doi = {10.48550/arXiv.2109.08002},
	abstract = {Neural embedding-based machine learning models have shown promise for predicting novel links in knowledge graphs. Unfortunately, their practical utility is diminished by their lack of interpretability. Recently, the fully interpretable, rule-based algorithm AnyBURL yielded highly competitive results on many general-purpose link prediction benchmarks. However, current approaches for aggregating predictions made by multiple rules are affected by redundancies. We improve upon AnyBURL by introducing the SAFRAN rule application framework, which uses a novel aggregation approach called Non-redundant Noisy-OR that detects and clusters redundant rules prior to aggregation. SAFRAN yields new state-of-the-art results for fully interpretable link prediction on the established general-purpose benchmarks FB15K-237, WN18RR and YAGO3-10. Furthermore, it exceeds the results of multiple established embedding-based algorithms on FB15K-237 and WN18RR and narrows the gap between rule-based and embedding-based algorithms on YAGO3-10.},
	urldate = {2025-09-30},
	publisher = {arXiv},
	author = {Ott, Simon and Meilicke, Christian and Samwald, Matthias},
	month = sep,
	year = {2021},
	note = {arXiv:2109.08002 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{pirro_relatedness_2020,
	title = {Relatedness and {TBox}-{Driven} {Rule} {Learning} in {Large} {Knowledge} {Bases}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5690},
	doi = {10.1609/aaai.v34i03.5690},
	abstract = {We present RARL, an approach to discover rules of the form body ⇒ head in large knowledge bases (KBs) that typically include a set of terminological facts (TBox) and a set of TBox-compliant assertional facts (ABox). RARL's main intuition is to learn rules by leveraging TBox-information and the semantic relatedness between the predicate(s) in the atoms of the body and the predicate in the head. RARL uses an efficient relatedness-driven TBox traversal algorithm, which given an input rule head, generates the set of most semantically related candidate rule bodies. Then, rule confidence is computed in the ABox based on a set of positive and negative examples. Decoupling candidate generation and rule quality assessment offers greater flexibility than previous work.},
	language = {en},
	number = {03},
	urldate = {2025-09-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pirrò, Giuseppe},
	month = apr,
	year = {2020},
	pages = {2975--2982},
}

@incollection{harth_fast_2020,
	address = {Cham},
	title = {Fast and {Exact} {Rule} {Mining} with {AMIE} 3},
	volume = {12123},
	isbn = {978-3-030-49460-5 978-3-030-49461-2},
	url = {https://link.springer.com/10.1007/978-3-030-49461-2_3},
	abstract = {Given a knowledge base (KB), rule mining ﬁnds rules such as “If two people are married, then they live (most likely) in the same place”. Due to the exponential search space, rule mining approaches still have diﬃculties to scale to today’s large KBs. In this paper, we present AMIE 3, a system that employs a number of sophisticated pruning strategies and optimizations. This allows the system to mine rules on large KBs in a matter of minutes. Most importantly, we do not have to resort to approximations or sampling, but are able to compute the exact conﬁdence and support of each rule. Our experiments on DBpedia, YAGO, and Wikidata show that AMIE 3 beats the state of the art by a factor of more than 15 in terms of runtime.},
	language = {en},
	urldate = {2025-09-29},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer International Publishing},
	author = {Lajus, Jonathan and Galárraga, Luis and Suchanek, Fabian},
	editor = {Harth, Andreas and Kirrane, Sabrina and Ngonga Ngomo, Axel-Cyrille and Paulheim, Heiko and Rula, Anisa and Gentile, Anna Lisa and Haase, Peter and Cochez, Michael},
	year = {2020},
	doi = {10.1007/978-3-030-49461-2_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {36--52},
}

@inproceedings{tran_fast_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Fast {Computation} of {Explanations} for {Inconsistency} in {Large}-{Scale} {Knowledge} {Graphs}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380014},
	doi = {10.1145/3366423.3380014},
	abstract = {Knowledge graphs (KGs) are essential resources for many applications including Web search and question answering. As KGs are often automatically constructed, they may contain incorrect facts. Detecting them is a crucial, yet extremely expensive task. Prominent solutions detect and explain inconsistency in KGs with respect to accompanying ontologies that describe the KG domain of interest. Compared to machine learning methods they are more reliable and human-interpretable but scale poorly on large KGs. In this paper, we present a novel approach to dramatically speed up the process of detecting and explaining inconsistency in large KGs by exploiting KG abstractions that capture prominent data patterns. Though much smaller, KG abstractions preserve inconsistency and their explanations. Our experiments with large KGs (e.g., DBpedia and Yago) demonstrate the feasibility of our approach and show that it significantly outperforms the popular baseline.},
	urldate = {2025-09-18},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria and Kharlamov, Evgeny and Strötgen, Jannik},
	month = apr,
	year = {2020},
	pages = {2613--2619},
}

@misc{nentidis_dealing_2025,
	title = {Dealing with {Inconsistency} for {Reasoning} over {Knowledge} {Graphs}: {A} {Survey}},
	shorttitle = {Dealing with {Inconsistency} for {Reasoning} over {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2502.19023},
	doi = {10.48550/arXiv.2502.19023},
	abstract = {In Knowledge Graphs (KGs), where the schema of the data is usually defined by particular ontologies, reasoning is a necessity to perform a range of tasks, such as retrieval of information, question answering, and the derivation of new knowledge. However, information to populate KGs is often extracted (semi-) automatically from natural language resources, or by integrating datasets that follow different semantic schemas, resulting in KG inconsistency. This, however, hinders the process of reasoning. In this survey, we focus on how to perform reasoning on inconsistent KGs, by analyzing the state of the art towards three complementary directions: a) the detection of the parts of the KG that cause the inconsistency, b) the fixing of an inconsistent KG to render it consistent, and c) the inconsistency-tolerant reasoning. We discuss existing work from a range of relevant fields focusing on how, and in which cases they are related to the above directions. We also highlight persisting challenges and future directions.},
	language = {en},
	urldate = {2025-09-18},
	publisher = {arXiv},
	author = {Nentidis, Anastasios and Akasiadis, Charilaos and Charalambidis, Angelos and Artikis, Alexander},
	month = feb,
	year = {2025},
	note = {arXiv:2502.19023 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{xiong_deeppath_2017,
	address = {Copenhagen, Denmark},
	title = {{DeepPath}: {A} {Reinforcement} {Learning} {Method} for {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{DeepPath}},
	url = {https://aclanthology.org/D17-1060/},
	doi = {10.18653/v1/D17-1060},
	abstract = {We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.},
	urldate = {2025-09-15},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
	editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
	month = sep,
	year = {2017},
	pages = {564--573},
}

@inproceedings{hubert_treat_2024,
	address = {Cham},
	title = {Treat {Different} {Negatives} {Differently}: {Enriching} {Loss} {Functions} with {Domain} and {Range} {Constraints} for {Link} {Prediction}},
	isbn = {978-3-031-60626-7},
	shorttitle = {Treat {Different} {Negatives} {Differently}},
	doi = {10.1007/978-3-031-60626-7_2},
	abstract = {Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that consider batches of true and false triples. However, different kinds of false triples exist and recent works suggest that they should not be valued equally, leading to specific negative sampling procedures. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. signatures of relations (domain and range) are high-quality negatives. Hence, we enrich the three main loss functions for link prediction such that all kinds of negatives are sampled but treated differently based on their semantic validity. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results which demonstrates both the generality and superiority of our proposed approach. In fact, the proposed loss functions (1) lead to better MRR and Hits@10 values, and (2) drive KGEMs towards better semantic correctness as measured by the Sem@K metric. This highlights that relation signatures globally improve KGEMs, and thus should be incorporated into loss functions. Domains and ranges of relations being largely available in schema-defined KGs, this makes our approach both beneficial and widely usable in practice.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Hubert, Nicolas and Monnin, Pierre and Brun, Armelle and Monticolo, Davy},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	year = {2024},
	keywords = {Knowledge Graph Embeddings, Link Prediction, Loss Functions, Schema-based Learning},
	pages = {22--40},
}

@inproceedings{suchanek_yago_2024,
	address = {Washington DC USA},
	title = {{YAGO} 4.5: {A} {Large} and {Clean} {Knowledge} {Base} with a {Rich} {Taxonomy}},
	isbn = {979-8-4007-0431-4},
	shorttitle = {{YAGO} 4.5},
	url = {https://dl.acm.org/doi/10.1145/3626772.3657876},
	doi = {10.1145/3626772.3657876},
	abstract = {Knowledge Bases (KBs) find applications in many knowledgeintensive tasks and, most notably, in information retrieval. Wikidata is one of the largest public general-purpose KBs. Yet, its collaborative nature has led to a convoluted schema and taxonomy. The YAGO 4 KB cleaned up the taxonomy by incorporating the ontology of Schema.org, resulting in a cleaner structure amenable to automated reasoning. However, it also cut away large parts of the Wikidata taxonomy, which is essential for information retrieval. In this paper, we extend YAGO 4 with a large part of the Wikidata taxonomy – while respecting logical constraints and the distinction between classes and instances. This yields YAGO 4.5, a new, logically consistent version of YAGO that adds a rich layer of informative classes. An intrinsic and an extrinsic evaluation show the value of the new resource.},
	language = {en},
	urldate = {2025-08-27},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Suchanek, Fabian M. and Alam, Mehwish and Bonald, Thomas and Chen, Lihu and Paris, Pierre-Henri and Soria, Jules},
	month = jul,
	year = {2024},
	pages = {131--140},
}

@article{wang_schema-aware_nodate,
	title = {Schema-aware {Iterative} {Completion} for {Knowledge} {Graphs} {Revisited}},
	abstract = {Recent success of knowledge graph (KG) has spurred widespread interests in methods for the problem of Knowledge Graph Completion (KGC). However, efforts to understand the quality of the candidate triples from these methods, in particular from the schema aspect, have been limited. In fact, most existing Knowledge Graph completion methods do not guarantee that the expanded Knowledge Graphs are consistent with the schema of the initial Knowledge Graph. Hence, schema-aware KGC seems to be way to go.},
	language = {en},
	author = {Wang, Fangrong and Bundy, Alan and Li, Xue and Nuamah, Kwabena and Xu, Lei and Mauceri, Stefano and Pan, Jeff Z},
}

@article{meznar_ontology_2022,
	title = {Ontology {Completion} with {Graph}-{Based} {Machine} {Learning}: {A} {Comprehensive} {Evaluation}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	shorttitle = {Ontology {Completion} with {Graph}-{Based} {Machine} {Learning}},
	url = {https://www.mdpi.com/2504-4990/4/4/56},
	doi = {10.3390/make4040056},
	abstract = {Increasing quantities of semantic resources offer a wealth of human knowledge, but their growth also increases the probability of wrong knowledge base entries. The development of approaches that identify potentially spurious parts of a given knowledge base is therefore highly relevant. We propose an approach for ontology completion that transforms an ontology into a graph and recommends missing edges using structure-only link analysis methods. By systematically evaluating thirteen methods (some for knowledge graphs) on eight different semantic resources, including Gene Ontology, Food Ontology, Marine Ontology, and similar ontologies, we demonstrate that a structure-only link analysis can offer a scalable and computationally efficient ontology completion approach for a subset of analyzed data sets. To the best of our knowledge, this is currently the most extensive systematic study of the applicability of different types of link analysis methods across semantic resources from different domains. It demonstrates that by considering symbolic node embeddings, explanations of the predictions (links) can be obtained, making this branch of methods potentially more valuable than black-box methods.},
	language = {en},
	number = {4},
	urldate = {2025-08-20},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Mežnar, Sebastian and Bevec, Matej and Lavrač, Nada and Škrlj, Blaž},
	month = dec,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {embedding, explainability, link prediction, machine learning, ontology completion},
	pages = {1107--1123},
}

@article{wiharja_schema_2020,
	title = {Schema aware iterative {Knowledge} {Graph} completion},
	volume = {65},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826820300494},
	doi = {10.1016/j.websem.2020.100616},
	abstract = {Recent success of Knowledge Graph has spurred widespread interests in methods for the problem of Knowledge Graph completion. However, efforts to understand the quality of the candidate triples from these methods, in particular from the schema aspect, have been limited. Indeed, most existing Knowledge Graph completion methods do not guarantee that the expanded Knowledge Graphs are consistent with the ontological schema of the initial Knowledge Graph. In this work, we challenge the silver standard method, by proposing the notion of schema-correctness. A fundamental challenge is how to make use of different types of Knowledge Graph completion methods together to improve the production of schema-correct triples. To address this, we analyse the characteristics of different methods and propose a schema aware iterative approach to Knowledge Graph completion. Our main findings are: (i) Some popular Knowledge Graph completion methods have surprisingly low schema-correctness ratio; (ii) Different types of Knowledge Graph completion methods can work with each other to help overcame individual limitations; (iii) Some iterative sequential combinations of Knowledge Graph completion methods have significantly better schema-correctness and coverage ratios than other combinations; (iv) All the MapReduce based iterative methods outperform involved single-pass methods significantly over the tested Knowledge Graphs in terms of productivity of schema-correct triples. Our findings and infrastructure can help further work on evaluating Knowledge Graph completion methods, more fine-grained approaches for schema aware iterative knowledge graph completion, as well as new approximate reasoning approaches based Knowledge Graph completion methods.},
	urldate = {2025-08-20},
	journal = {Journal of Web Semantics},
	author = {Wiharja, Kemas and Pan, Jeff Z. and Kollingbaum, Martin J. and Deng, Yu},
	month = dec,
	year = {2020},
	keywords = {Approximate reasoning, Correctness and coverage, Knowledge Graph completion, Knowledge Graph reasoning, SHACL constraint, Schema aware},
	pages = {100616},
}

@article{wu_rule_2023,
	title = {Rule {Learning} over {Knowledge} {Graphs}: {A} {Review}},
	volume = {1},
	copyright = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/openAccess},
	issn = {2942-7517},
	shorttitle = {Rule {Learning} over {Knowledge} {Graphs}},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/TGDK.1.1.7},
	doi = {10.4230/TGDK.1.1.7},
	abstract = {Compared to black-box neural networks, logic rules express explicit knowledge, can provide human-understandable explanations for reasoning processes, and have found their wide application in knowledge graphs and other downstream tasks. As extracting rules manually from large knowledge graphs is labour-intensive and often infeasible, automated rule learning has recently attracted significant interest, and a number of approaches to rule learning for knowledge graphs have been proposed. This survey aims to provide a review of approaches and a classification of state-of-the-art systems for learning first-order logic rules over knowledge graphs. A comparative analysis of various approaches to rule learning is conducted based on rule language biases, underlying methods, and evaluation metrics. The approaches we consider include inductive logic programming (ILP)-based, statistical path generalisation, and neuro-symbolic methods. Moreover, we highlight important and promising application scenarios of rule learning, such as rule-based knowledge graph completion, fact checking, and applications in other research areas.},
	language = {en},
	number = {1},
	urldate = {2025-08-18},
	journal = {Transactions on Graph Data and Knowledge (TGDK)},
	author = {Wu, Hong and Wang, Zhe and Wang, Kewen and Omran, Pouya Ghiasnezhad and Li, Jiangmeng},
	editor = {Hogan, Aidan and Horrocks, Ian and Hotho, Andreas and Kagal, Lalana},
	year = {2023},
	note = {Artwork Size: 23 pages, 1239675 bytes
Medium: application/pdf
Publisher: Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	keywords = {Computing methodologies → Knowledge representation and reasoning, Information systems → Data mining, Knowledge graphs, Link prediction, Rule learning},
	pages = {7:1--7:23},
}

@article{lisi_combining_nodate,
	title = {Combining {Rule} {Learning} and {Nonmonotonic} {Reasoning} for {Link} {Prediction} in {Knowledge} {Graphs}},
	abstract = {Learning rules from knowledge graphs (KG) is a crucial task for KG completion, cleaning and curation. The majority of existing approaches are capable of learning only Horn rules from KGs, which are, however, insufﬁciently expressive for capturing exceptions and thus might make incorrect predictions on missing facts. In this paper we discuss our recent progress in addressing this limitation. More speciﬁcally, we report the challenges of learning rules with exceptions, brieﬂy describe our recent approach which combines rule learning and nonmonotonic reasoning and outline the ongoing and future research directions.},
	language = {en},
	author = {Lisi, Francesca A and Stepanova, Daria},
}

@inproceedings{wu_learning_2022,
	address = {Haifa, Israel},
	title = {Learning {Typed} {Rules} over {Knowledge} {Graphs}},
	isbn = {978-1-956792-01-0},
	url = {https://proceedings.kr.org/2022/51},
	doi = {10.24963/kr.2022/51},
	abstract = {Rule learning from large datasets has regained extensive interest as rules are useful for developing explainable approaches to many applications in knowledge graphs. However, existing methods for rule learning are still limited in terms of rule expressivity and rule quality. This paper presents a new method for learning typed rules by employing type information. Our experimental evaluation shows the superiority of our system compared to state-of-the-art rule learners. In particular, we demonstrate the usefulness of typed rules in reasoning for link prediction.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wu, Hong and Wang, Zhe and Wang, Kewen and Shen, Yi-Dong},
	month = jul,
	year = {2022},
	pages = {494--503},
}

@inproceedings{ott_rule-based_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Rule-based {Knowledge} {Graph} {Completion} with {Canonical} {Models}},
	isbn = {979-8-4007-0124-5},
	url = {https://dl.acm.org/doi/10.1145/3583780.3615042},
	doi = {10.1145/3583780.3615042},
	abstract = {Rule-based approaches have proven to be an efficient and explainable method for knowledge base completion. Their predictive quality is on par with classic knowledge graph embedding models such as TransE or ComplEx, however, they cannot achieve the results of neural models proposed recently. The performance of a rule-based approach depends crucially on the solution of the rule aggregation problem, which is concerned with the computation of a score for a prediction that is generated by several rules. Within this paper, we propose a supervised approach to learn a reweighted confidence value for each rule to get an optimal explanation for the training set given a specific aggregation function. In particular, we apply our approach to two aggregation functions: We learn weights for a noisy-or multiplication and apply logistic regression, which computes the score of a prediction as a sum of these weights. Due to the simplicity of both models the final score is fully explainable. Our experimental results show that we can significantly improve the predictive quality of a rule-based approach. We compare our method with current state-of-the-art latent models that lack explainability, and achieve promising results.},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ott, Simon and Betz, Patrick and Stepanova, Daria and Gad-Elrab, Mohamed H. and Meilicke, Christian and Stuckenschmidt, Heiner},
	month = oct,
	year = {2023},
	pages = {1971--1981},
}

@inproceedings{tran_towards_2017,
	address = {Cham},
	title = {Towards {Nonmonotonic} {Relational} {Learning} from {Knowledge} {Graphs}},
	isbn = {978-3-319-63342-8},
	doi = {10.1007/978-3-319-63342-8_8},
	abstract = {Recent advances in information extraction have led to the so-called knowledge graphs (KGs), i.e., huge collections of relational factual knowledge. Since KGs are automatically constructed, they are inherently incomplete, thus naturally treated under the Open World Assumption (OWA). Rule mining techniques have been exploited to support the crucial task of KG completion. However, these techniques can mine Horn rules, which are insufficiently expressive to capture exceptions, and might thus make incorrect predictions on missing links. Recently, a rule-based method for filling in this gap was proposed which, however, applies to a flattened representation of a KG with only unary facts. In this work we make the first steps towards extending this approach to KGs in their original relational form, and provide preliminary evaluation results on real-world KGs, which demonstrate the effectiveness of our method.},
	language = {en},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Springer International Publishing},
	author = {Tran, Hai Dang and Stepanova, Daria and Gad-Elrab, Mohamed H. and Lisi, Francesca A. and Weikum, Gerhard},
	editor = {Cussens, James and Russo, Alessandra},
	year = {2017},
	keywords = {Horn Rules, Nominal Exclusion, Nonmonotonic Rule, Relational Association Rule Mining, Theory Revision},
	pages = {94--107},
}

@inproceedings{safavi_evaluating_2020,
	address = {Online},
	title = {Evaluating the {Calibration} of {Knowledge} {Graph} {Embeddings} for {Trustworthy} {Link} {Prediction}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.667},
	doi = {10.18653/v1/2020.emnlp-main.667},
	abstract = {Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output conﬁdence scores that reﬂect the expected correctness of predicted knowledge graph triples. We ﬁrst conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner’s perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.},
	language = {en},
	urldate = {2025-08-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Safavi, Tara and Koutra, Danai and Meij, Edgar},
	year = {2020},
	pages = {8308--8321},
}

@inproceedings{jain_improving_2021,
	address = {Cham},
	title = {Improving {Knowledge} {Graph} {Embeddings} with {Ontological} {Reasoning}},
	isbn = {978-3-030-88361-4},
	doi = {10.1007/978-3-030-88361-4_24},
	abstract = {Knowledge graph (KG) embedding models have emerged as powerful means for KG completion. To learn the representation of KGs, entities and relations are projected in a low-dimensional vector space so that not only existing triples in the KG are preserved but also new triples can be predicted. Embedding models might learn a good representation of the input KG, but due to the nature of machine learning approaches, they often lose the semantics of entities and relations, which might lead to nonsensical predictions. To address this issue we propose to improve the accuracy of embeddings using ontological reasoning. More specifically, we present a novel iterative approach ReasonKGE that identifies dynamically via symbolic reasoning inconsistent predictions produced by a given embedding model and feeds them as negative samples for retraining this model. In order to address the scalability problem that arises when integrating ontological reasoning into the training process, we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining. Experimental results demonstrate the improvements in accuracy of facts produced by our method compared to the state-of-the-art.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2021},
	publisher = {Springer International Publishing},
	author = {Jain, Nitisha and Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria},
	editor = {Hotho, Andreas and Blomqvist, Eva and Dietze, Stefan and Fokoue, Achille and Ding, Ying and Barnaghi, Payam and Haller, Armin and Dragoni, Mauro and Alani, Harith},
	year = {2021},
	pages = {410--426},
}

@inproceedings{hao_universal_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Universal {Representation} {Learning} of {Knowledge} {Bases} by {Jointly} {Embedding} {Instances} and {Ontological} {Concepts}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330838},
	doi = {10.1145/3292500.3330838},
	abstract = {Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding ontological concepts connected via a (small) set of cross-view links. Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology-view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance.},
	urldate = {2025-07-31},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun, Yizhou and Wang, Wei},
	month = jul,
	year = {2019},
	pages = {1709--1719},
}

@inproceedings{wiharja_more_2018,
	address = {Cham},
	title = {More {Is} {Better}: {Sequential} {Combinations} of {Knowledge} {Graph} {Embedding} {Approaches}},
	isbn = {978-3-030-04284-4},
	shorttitle = {More {Is} {Better}},
	doi = {10.1007/978-3-030-04284-4_2},
	abstract = {Constructing and maintaining large-scale good quality knowledge graphs present many challenges. Knowledge graph completion has been regarded a promising direction in the knowledge graph community. The majority of current work for knowledge graph completion approaches do not take the schema of a target knowledge graph as input. As a result, the triples generated by these approaches are not necessarily consistent with the schema of the target knowledge graph. This paper proposes to improve the correctness of knowledge graph completion based on Schema Aware Triple Classification (SATC), which enables sequential combinations of knowledge graph embedding approaches. Extensive experiments show that our proposed approaches can significantly improve the correctness of the new triples produced by knowledge graph embedding methods.},
	language = {en},
	booktitle = {Semantic {Technology}},
	publisher = {Springer International Publishing},
	author = {Wiharja, Kemas and Pan, Jeff Z. and Kollingbaum, Martin and Deng, Yu},
	editor = {Ichise, Ryutaro and Lecue, Freddy and Kawamura, Takahiro and Zhao, Dongyan and Muggleton, Stephen and Kozaki, Kouji},
	year = {2018},
	keywords = {Approximate reasoning, Artificial Intelligence, Embedding, Knowledge graph, Knowledge representation and reasoning, Schema aware triple classification},
	pages = {19--35},
}

@misc{suresh_hybrid_2020,
	title = {A {Hybrid} {Model} for {Learning} {Embeddings} and {Logical} {Rules} {Simultaneously} from {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2009.10800},
	doi = {10.48550/arXiv.2009.10800},
	abstract = {The problem of knowledge graph (KG) reasoning has been widely explored by traditional rule-based systems and more recently by knowledge graph embedding methods. While logical rules can capture deterministic behavior in a KG they are brittle and mining ones that infer facts beyond the known KG is challenging. Probabilistic embedding methods are effective in capturing global soft statistical tendencies and reasoning with them is computationally efﬁcient. While embedding representations learned from rich training data are expressive, incompleteness and sparsity in real-world KGs can impact their effectiveness. We aim to leverage the complementary properties of both methods to develop a hybrid model that learns both high-quality rules and embeddings simultaneously. Our method uses a cross feedback paradigm wherein, an embedding model is used to guide the search of a rule mining system to mine rules and infer new facts. These new facts are sampled and further used to reﬁne the embedding model. Experiments on multiple benchmark datasets show the effectiveness of our method over other competitive standalone and hybrid baselines. We also show its efﬁcacy in a sparse KG setting and ﬁnally explore the connection with negative sampling.},
	language = {en},
	urldate = {2025-07-09},
	publisher = {arXiv},
	author = {Suresh, Susheel and Neville, Jennifer},
	month = sep,
	year = {2020},
	note = {arXiv:2009.10800 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tanon_completeness-aware_nodate,
	title = {Completeness-aware {Rule} {Learning} from {Knowledge} {Graphs}},
	abstract = {Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts that are widely used in entity recognition, structured search, question answering, and other tasks. Rule mining is commonly applied to discover patterns in KGs. However, unlike in traditional association rule mining, KGs provide a setting with a high degree of incompleteness, which may result in the wrong estimation of the quality of mined rules, leading to erroneous beliefs such as all artists have won an award. In this paper we propose to use (in-)completeness meta-information to better assess the quality of rules learned from incomplete KGs. We introduce completeness-aware scoring functions for relational association rules. Experimental evaluation both on real and synthetic datasets shows that the proposed rule ranking approaches have remarkably higher accuracy than the state-of-the-art methods in uncovering missing facts.},
	language = {en},
	author = {Tanon, Thomas Pellissier and Stepanova, Daria and Razniewski, Simon and Mirza, Paramita and Weikum, Gerhard},
}

@inproceedings{zhu_closer_2023,
	address = {New York, NY, USA},
	series = {{IJCKG} '22},
	title = {A {Closer} {Look} at {Probability} {Calibration} of {Knowledge} {Graph} {Embedding}},
	isbn = {978-1-4503-9987-6},
	url = {https://dl.acm.org/doi/10.1145/3579051.3579072},
	doi = {10.1145/3579051.3579072},
	abstract = {When the estimated probabilities do not match the relative frequencies, we say these estimated probabilities are uncalibrated [39], which may cause incorrect decision making, and is particularly undesired in high-stakes tasks [45]. Knowledge Graph embedding models are reported to produce uncalibrated probabilities [36], e.g., for all the triples predicted with probability 0.9, the percentage of them being truly correct triples is not . In this article, we take a closer look at this problem. First, we confirmed the issue that typical KG Embedding models are uncalibrated. Then, we show how off-the-shelf calibration techniques can be used to mitigate this issue, among which binning-based calibration produces more calibrated probabilities. We also investigated the possible reasons for the uncalibrated probabilities and found that the expit transform, the way used to convert embedding scores into probabilities, is ineffective in most cases.},
	urldate = {2025-06-16},
	booktitle = {Proceedings of the 11th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Ruiqi and Wang, Fangrong and Bundy, Alan and Li, Xue and Nuamah, Kwabena and Xu, Lei and Mauceri, Stefano and Pan, Jeff Z.},
	month = feb,
	year = {2023},
	pages = {104--109},
}

@inproceedings{rao_using_2024,
	address = {Singapore Singapore},
	title = {Using {Model} {Calibration} to {Evaluate} {Link} {Prediction} in {Knowledge} {Graphs}},
	isbn = {979-8-4007-0171-9},
	url = {https://dl.acm.org/doi/10.1145/3589334.3645506},
	doi = {10.1145/3589334.3645506},
	abstract = {Link prediction models assign scores to predict new, plausible edges to complete knowledge graphs. In link prediction evaluation, the score of an existing edge (positive) is ranked w.r.t. the scores of its synthetically corrupted counterparts (negatives). An accurate model ranks positives higher than negatives, assuming ascending order. Since the number of negatives are typically large for a single positive, link prediction evaluation is computationally expensive. As far as we know, only one approach has proposed to replace rank aggregations by a distance between sample positives and negatives. Unfortunately, the distance does not consider individual ranks, so edges in isolation cannot be assessed. In this paper, we propose an alternative protocol based on posterior probabilities of positives rather than ranks. A calibration function assigns posterior probabilities to edges that measure their plausibility. We propose to assess our alternative protocol in various ways, including whether expected semantics are captured when using different strategies to synthetically generate negatives. Our experiments show that posterior probabilities and ranks are highly correlated. Also, the time reduction of our alternative protocol is quite significant: more than 77\% compared to rank-based evaluation. We conclude that link prediction evaluation based on posterior probabilities is viable and significantly reduces computational costs.},
	language = {en},
	urldate = {2025-06-04},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {ACM},
	author = {Rao, Aishwarya and Krishnan, Narayanan Asuri and Rivero, Carlos R.},
	month = may,
	year = {2024},
	pages = {2042--2051},
}

@article{meilicke_anytime_2024,
	title = {Anytime bottom-up rule learning for large-scale knowledge graph completion},
	volume = {33},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-023-00800-5},
	doi = {10.1007/s00778-023-00800-5},
	abstract = {Knowledge graph completion is the task of predicting correct facts that can be expressed by the vocabulary of a given knowledge graph, which are not explicitly stated in that graph. Broadly, there are two main approaches for solving the knowledge graph completion problem. Sub-symbolic approaches embed the nodes and/or edges of a given graph into a low-dimensional vector space and use a scoring function to determine the plausibility of a given fact. Symbolic approaches learn a model that remains within the primary representation of the given knowledge graph. Rule-based approaches are well-known examples. One such approach is AnyBURL. It works by sampling random paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is close to current state of the art with the additional benefit of offering an explanation for a predicted fact. In this paper, we propose several improvements and extensions of AnyBURL. In particular, we focus on AnyBURL’s capability to be successfully applied to large and very large datasets. Overall, we propose four separate extensions: (i) We add to each rule a set of pairwise inequality constraints which enforces that different variables cannot be grounded by the same entities, which results into more appropriate confidence estimations. (ii) We introduce reinforcement learning to guide path sampling in order to use available computational resources more efficiently. (iii) We propose an efficient sampling strategy to approximate the confidence of a rule instead of computing its exact value. (iv) We develop a new multithreaded AnyBURL, which incorporates all previously mentioned modifications. In an experimental study, we show that our approach outperforms both symbolic and sub-symbolic approaches in large-scale knowledge graph completion. It has a higher prediction quality and requires significantly less time and computational resources.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {The VLDB Journal},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Betz, Patrick and Fink, Manuel and Stuckeschmidt, Heiner},
	month = jan,
	year = {2024},
	keywords = {Knowledge graph completion, Link prediction, Rule learning},
	pages = {131--161},
}

@article{friedman_symbolic_nodate,
	title = {Symbolic {Querying} of {Vector} {Spaces}: {Probabilistic} {Databases} {Meets} {Relational} {Embeddings}},
	abstract = {We propose unifying techniques from probabilistic databases and relational embedding models with the goal of performing complex queries on incomplete and uncertain data. We formalize a probabilistic database model with respect to which all queries are done. This allows us to leverage the rich literature of theory and algorithms from probabilistic databases for solving problems. While this formalization can be used with any relational embedding model, the lack of a well-deﬁned joint probability distribution causes simple query problems to become provably hard. With this in mind, we introduce TRACTOR, a relational embedding model designed to be a tractable probabilistic database, by exploiting typical embedding assumptions within the probabilistic framework. Using a principled, efﬁcient inference algorithm that can be derived from its deﬁnition, we empirically demonstrate that TRACTOR is an effective and general model for these querying tasks.},
	language = {en},
	author = {Friedman, Tal},
}

@article{loconte_how_nodate,
	title = {How to {Turn} {Your} {Knowledge} {Graph} {Embeddings} into {Generative} {Models}},
	abstract = {Some of the most successful knowledge graph embedding (KGE) models for link prediction – CP, RESCAL, TUCKER, COMPLEX – can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits – constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.},
	language = {en},
	author = {Loconte, Lorenzo and Mauro, Nicola Di},
}

@misc{tabacof_probability_2020,
	title = {Probability {Calibration} for {Knowledge} {Graph} {Embedding} {Models}},
	url = {http://arxiv.org/abs/1912.10000},
	doi = {10.48550/arXiv.1912.10000},
	abstract = {Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get signiﬁcantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to deﬁne relation-speciﬁc decision thresholds.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Tabacof, Pedro and Costabello, Luca},
	month = feb,
	year = {2020},
	note = {arXiv:1912.10000 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	doi = {10.48550/arXiv.1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	urldate = {2025-05-27},
	publisher = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{islam_negative_2022,
	title = {Negative sampling and rule mining for explainable link prediction in knowledge graphs},
	volume = {250},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122005342},
	doi = {10.1016/j.knosys.2022.109083},
	abstract = {Several KG embedding methods were proposed to learn low dimensional vector representations of entities and relations of a KG. Such representations facilitate the link prediction task, in the service of inference and KG completion. In this context, it is important to achieve both an efficient KG embedding and explainable predictions. During learning of efficient embeddings, sampling negative triples was highlighted as an important step as KGs only have observed positive triples. We propose an efficient simple negative sampling (SNS) method based on the assumption that the entities which are closer in the embedding space to the corrupted entity are able to provide high-quality negative triples. As for explainability, it actually constitutes a thriving research question especially when it comes to analyze KGs with their rich semantics rooted in description logics. Hence, we propose in this paper a new rule mining method on the basis of learned embeddings. We extensively evaluate our proposals through several experiments. We evaluate our SNS sampling method plugged to several KG embedding models through link prediction task performances on well-known datasets. Experimental results show that the SNS improves the prediction performance of KG embedding models, and outperforms the existing sampling methods. To assess the performance of our rule mining method with and without SNS, we mine and evaluate rules on three popular datasets. The extracted rules are evaluated as knowledge nuggets extracted from the KG and also as support for explainable link prediction. The overall results are good and open the way to many improvements and new perspectives.},
	urldate = {2025-05-27},
	journal = {Knowledge-Based Systems},
	author = {Islam, Md Kamrul and Aridhi, Sabeur and Smail-Tabbone, Malika},
	month = aug,
	year = {2022},
	keywords = {Explainability, Knowledge graph embedding, Link prediction, Negative sampling, Rule mining},
	pages = {109083},
}

@inproceedings{zhang_iteratively_2019,
	address = {San Francisco CA USA},
	title = {Iteratively {Learning} {Embeddings} and {Rules} for {Knowledge} {Graph} {Reasoning}},
	isbn = {978-1-4503-6674-8},
	url = {https://dl.acm.org/doi/10.1145/3308558.3313612},
	doi = {10.1145/3308558.3313612},
	abstract = {Reasoning is essential for the development of large knowledge graphs, especially for completion, which aims to infer new triples based on existing ones. Both rules and embeddings can be used for knowledge graph reasoning and they have their own advantages and difficulties. Rule-based reasoning is accurate and explainable but rule learning with searching over the graph always suffers from efficiency due to huge search space. Embedding-based reasoning is more scalable and efficient as the reasoning is conducted via computation between embeddings, but it has difficulty learning good representations for sparse entities because a good embedding relies heavily on data richness. Based on this observation, in this paper we explore how embedding and rule learning can be combined together and complement each other’s difficulties with their advantages. We propose a novel framework IterE iteratively learning embeddings and rules, in which rules are learned from embeddings with proper pruning strategy and embeddings are learned from existing triples and new triples inferred by rules. Evaluations on embedding qualities of IterE show that rules help improve the quality of sparse entity embeddings and their link prediction results. We also evaluate the efficiency of rule learning and quality of rules from IterE compared with AMIE+, showing that IterE is capable of generating high quality rules more efficiently. Experiments show that iteratively learning embeddings and rules benefit each other during learning and prediction.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {ACM},
	author = {Zhang, Wen and Paudel, Bibek and Wang, Liang and Chen, Jiaoyan and Zhu, Hai and Zhang, Wei and Bernstein, Abraham and Chen, Huajun},
	month = may,
	year = {2019},
	pages = {2366--2377},
}

@article{omran_embedding-based_2021,
	title = {An {Embedding}-{Based} {Approach} to {Rule} {Learning} in {Knowledge} {Graphs}},
	volume = {33},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/8839576/},
	doi = {10.1109/TKDE.2019.2941685},
	abstract = {It is natural and effective to use rules for representing explicit knowledge in knowledge graphs. However, it is challenging to learn rules automatically from very large knowledge graphs such as Freebase and YAGO. This paper presents a new approach, RLvLR (Rule Learning via Learning Representations), to learning rules from large knowledge graphs by using the technique of embedding in representation learning together with a new sampling method. Based on RLvLR, a new method RLvLR-Stream is developed for learning rules from streams of knowledge graphs. Both RLvLR and RLvLR-Stream have been implemented and experiments conducted to validate the proposed methods regarding the tasks of rule learning and link prediction. Experimental results show that our systems are able to handle the task of rule learning from large knowledge graphs with high accuracy and outperform some state-of-the-art systems. Specifically, for massive knowledge graphs with hundreds of predicates and over 10M facts, RLvLR is much faster and can learn much more quality rules than major systems for rule learning in knowledge graphs such as AMIE+. In the setting of knowledge graph streams, RLvLR-Stream significantly improved RLvLR for both rule learning and link prediction.},
	number = {4},
	urldate = {2025-05-27},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Omran, Pouya Ghiasnezhad and Wang, Kewen and Wang, Zhe},
	month = apr,
	year = {2021},
	keywords = {Knowledge based systems, Knowledge engineering, Predictive models, Rule learning, Sampling methods, Scalability, Standards, Task analysis, knowledge graphs},
	pages = {1348--1359},
}

@incollection{damato_rule_2018,
	address = {Cham},
	title = {Rule {Induction} and {Reasoning} over {Knowledge} {Graphs}},
	volume = {11078},
	isbn = {978-3-030-00337-1 978-3-030-00338-8},
	url = {https://link.springer.com/10.1007/978-3-030-00338-8_6},
	abstract = {Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. Learning rules from KGs is a crucial task for KG completion, cleaning and curation. This tutorial presents state-ofthe-art rule induction methods, recent advances, research opportunities as well as open challenges along this avenue. We put a particular emphasis on the problems of learning exception-enriched rules from highly biased and incomplete data. Finally, we discuss possible extensions of classical rule induction techniques to account for unstructured resources (e.g., text) along with the structured ones.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {Reasoning {Web}. {Learning}, {Uncertainty}, {Streaming}, and {Scalability}},
	publisher = {Springer International Publishing},
	author = {Stepanova, Daria and Gad-Elrab, Mohamed H. and Ho, Vinh Thinh},
	editor = {d’Amato, Claudia and Theobald, Martin},
	year = {2018},
	doi = {10.1007/978-3-030-00338-8_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {142--172},
}

@incollection{vrandecic_rule_2018,
	address = {Cham},
	title = {Rule {Learning} from {Knowledge} {Graphs} {Guided} by {Embedding} {Models}},
	volume = {11136},
	isbn = {978-3-030-00670-9 978-3-030-00671-6},
	url = {https://link.springer.com/10.1007/978-3-030-00671-6_5},
	abstract = {Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as conﬁdence reﬂect rule quality well when the KG is reasonably complete; however, these measures might be misleading otherwise. So it is difﬁcult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules is generated. Therefore, the ranking and pruning of candidate rules is a major problem. To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {The {Semantic} {Web} – {ISWC} 2018},
	publisher = {Springer International Publishing},
	author = {Ho, Vinh Thinh and Stepanova, Daria and Gad-Elrab, Mohamed H. and Kharlamov, Evgeny and Weikum, Gerhard},
	editor = {Vrandečić, Denny and Bontcheva, Kalina and Suárez-Figueroa, Mari Carmen and Presutti, Valentina and Celino, Irene and Sabou, Marta and Kaffee, Lucie-Aimée and Simperl, Elena},
	year = {2018},
	doi = {10.1007/978-3-030-00671-6_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {72--90},
}

@article{gad-elrab_explainable_nodate,
	title = {Explainable {Methods} for {Knowledge} {Graph} {Reﬁnement} and {Exploration} via {Symbolic} {Reasoning}},
	language = {en},
	author = {Gad-Elrab, Mohamed H},
}

@inproceedings{gad-elrab_exception-enriched_2016,
	address = {Cham},
	title = {Exception-{Enriched} {Rule} {Learning} from {Knowledge} {Graphs}},
	isbn = {978-3-319-46523-4},
	doi = {10.1007/978-3-319-46523-4_15},
	abstract = {Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. To overcome this problem, we present a method for effective revision of learned Horn rules by adding exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2016},
	publisher = {Springer International Publishing},
	author = {Gad-Elrab, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
	editor = {Groth, Paul and Simperl, Elena and Gray, Alasdair and Sabou, Marta and Krötzsch, Markus and Lecue, Freddy and Flöck, Fabian and Gil, Yolanda},
	year = {2016},
	keywords = {Horn Rules, Knowledge Graph (KG), Nonmonotonic Rule, Wikidata, Witness Set},
	pages = {234--251},
}
