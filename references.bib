
@article{dessi_cs-kg_2025,
	title = {{CS}-{KG} 2.0: {A} {Large}-scale {Knowledge} {Graph} of {Computer} {Science}},
	volume = {12},
	copyright = {2025 The Author(s)},
	issn = {2052-4463},
	shorttitle = {{CS}-{KG} 2.0},
	url = {https://www.nature.com/articles/s41597-025-05200-8},
	doi = {10.1038/s41597-025-05200-8},
	abstract = {The rapid evolution of AI and the increased accessibility of scientific articles through open access marks a pivotal moment in research. AI-driven tools are reshaping how scientists explore, interpret, and contribute to the body of scientific knowledge, offering unprecedented opportunities. Nonetheless, a significant challenge remains: dealing with the overwhelming number of papers published every year. A promising solution is the use of knowledge graphs, which provide structured, interconnected, and formalized frameworks that improve the capabilities of AI systems to integrate information from the literature. This paper presents the last version of the Computer Science Knowledge Graph (CS-KG 2.0), an extensive knowledge base generated from 15 million research papers. CS-KG 2.0 describes 25 million entities linked by 67 million relationships, offering a nuanced representation of the scientific knowledge within the field of computer science. This innovative resource facilitates new research opportunities in key areas such as analysis and forecasting of research trends, hypothesis generation, smart literature search, automatic production of literature review, and scientific question-answering.},
	language = {en},
	number = {1},
	urldate = {2025-12-02},
	journal = {Scientific Data},
	author = {Dess√≠, Danilo and Osborne, Francesco and Buscaldi, Davide and Reforgiato Recupero, Diego and Motta, Enrico},
	month = jun,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Research data, Technology},
	pages = {964},
}

@article{zeb_complex_2022,
	title = {Complex graph convolutional network for link prediction in knowledge graphs},
	volume = {200},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422002548},
	doi = {10.1016/j.eswa.2022.116796},
	abstract = {Knowledge graph (KG) embedding models map nodes and edges to fixed-length vectors and obtain the similarity of nodes as the output of a scoring function to predict missing links between nodes. KG embedding methods based on graph convolutional networks (GCNs) have recently gained significant attention due to their ability to add information of neighboring nodes into the nodes‚Äô embeddings. However, existing GCNs are primarily based on real-valued embeddings, which have high distortion, particularly when modeling graphs with varying geometric structures. In this paper, we propose complex graph convolutional network (ComplexGCN), a novel extension of the standard GCNs in complex space to combine the expressiveness of complex geometry with GCNs for improving the representation quality of KG components. The proposed ComplexGCN comprises a set of complex graph convolutional layers and a complex scoring function based on PARATUCK2 decomposition: the former includes information of neighboring nodes into the nodes‚Äô embeddings, while the latter leverages these embeddings to predict new links between nodes. The proposed model demonstrates enhanced performance compared to existing methods on the two recent standard link prediction datasets.},
	urldate = {2025-12-02},
	journal = {Expert Systems with Applications},
	author = {Zeb, Adnan and Saif, Summaya and Chen, Junde and Haq, Anwar Ul and Gong, Zhiguo and Zhang, Defu},
	month = aug,
	year = {2022},
	keywords = {Complex embeddings, Graph convolutional network, Knowledge graph, Link prediction, Tensor decomposition},
	pages = {116796},
}

@misc{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {https://ui.adsabs.harvard.edu/abs/2016arXiv160902907K},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2025-12-02},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = sep,
	year = {2016},
	note = {ADS Bibcode: 2016arXiv160902907K},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{arrar_comprehensive_2024,
	title = {A comprehensive survey of link prediction methods},
	volume = {80},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-023-05591-8},
	doi = {10.1007/s11227-023-05591-8},
	abstract = {Link prediction aims to anticipate the probability of a future connection between two nodes in a given network based on their previous interactions and the network structure. Link prediction is a rapidly evolving field of research that has attracted interest from physicists and computer scientists. Over the years, numerous methods have been developed for link prediction, encompassing similarity-based indices, machine learning techniques, and more. While existing surveys have covered link prediction research until 2020, there has been a substantial surge in research activities in recent years, particularly between 2021 and 2023. This increased interest underscores the pressing need to comprehensively explore the latest advancements and approaches in link prediction. We analyse and present the most notable research from 2018 to 2023. Our goal is to offer a comprehensive overview of the recent developments in the field. Besides summarizing and presenting previous experimental results, our survey offers a comprehensive analysis highlighting the strengths and limitations of various link prediction methods.},
	language = {en},
	number = {3},
	urldate = {2025-12-02},
	journal = {The Journal of Supercomputing},
	author = {Arrar, Djihad and Kamel, Nadjet and Lakhfif, Abdelaziz},
	month = feb,
	year = {2024},
	keywords = {Graph neural network, Link prediction, Machine learning},
	pages = {3902--3942},
}

@inproceedings{li_evaluating_2023,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Evaluating graph neural networks for link prediction: current pitfalls and new benchmarking},
	shorttitle = {Evaluating graph neural networks for link prediction},
	abstract = {Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT.},
	urldate = {2025-12-02},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Li, Juanhui and Shomer, Harry and Mao, Haitao and Zeng, Shenglai and Ma, Yao and Shah, Neil and Tang, Jiliang and Yin, Dawei},
	month = dec,
	year = {2023},
	pages = {3853--3866},
}

@article{zeng_logical_2023,
	title = {Logical {Rule}-{Based} {Knowledge} {Graph} {Reasoning}: {A} {Comprehensive} {Survey}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	shorttitle = {Logical {Rule}-{Based} {Knowledge} {Graph} {Reasoning}},
	url = {https://www.mdpi.com/2227-7390/11/21/4486},
	doi = {10.3390/math11214486},
	abstract = {With its powerful expressive capability and intuitive presentation, the knowledge graph has emerged as one of the primary forms of knowledge represent...},
	language = {en},
	number = {21},
	urldate = {2025-12-02},
	journal = {Mathematics},
	author = {Zeng, Zefan and Cheng, Qing and Si, Yuehang and Zeng, Zefan and Cheng, Qing and Si, Yuehang},
	month = oct,
	year = {2023},
	note = {Company: Multidisciplinary Digital Publishing Institute
Distributor: Multidisciplinary Digital Publishing Institute
Institution: Multidisciplinary Digital Publishing Institute
Label: Multidisciplinary Digital Publishing Institute
Publisher: publisher},
	keywords = {embedding, inductive logic programming, knowledge graph reasoning, logical rules, neural network, probabilistic graph},
}

@inproceedings{sun_rotate_2018,
	title = {{RotatE}: {Knowledge} {Graph} {Embedding} by {Relational} {Rotation} in {Complex} {Space}},
	shorttitle = {{RotatE}},
	url = {https://openreview.net/forum?id=HkgEQnRqYQ},
	abstract = {We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.},
	language = {en},
	urldate = {2025-12-02},
	author = {Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
	month = sep,
	year = {2018},
}

@inproceedings{trouillon_complex_2016,
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	url = {https://proceedings.mlr.press/v48/trouillon16.html},
	abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
	language = {en},
	urldate = {2025-12-02},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Trouillon, Th√©o and Welbl, Johannes and Riedel, Sebastian and Gaussier, Eric and Bouchard, Guillaume},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {2071--2080},
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
	abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
	urldate = {2025-12-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	year = {2013},
}

@book{dean_owl_2004,
	title = {{OWL} {Web} {Ontology} {Language} {Reference}},
	publisher = {World Wide Web Consortium},
	author = {Dean, M. and Schreiber, A.T. and Bechofer, S. and van Harmelen, F.A.H. and Hendler, J. and Horrocks, I. and MacGuinness, D. and Patel-Schneider, P. and Stein, L. A.},
	year = {2004},
}

@article{dettmers_convolutional_2018,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11573},
	doi = {10.1609/aaai.v32i1.11573},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models ‚Äî which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree ‚Äî which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set ‚Äî however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets ‚Äî deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.},
	language = {en},
	number = {1},
	urldate = {2025-11-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = apr,
	year = {2018},
	keywords = {convolution},
}

@inproceedings{lisi_combining_2017,
	title = {Combining {Rule} {Learning} and {Nonmonotonic} {Reasoning} for {Link} {Prediction} in {Knowledge} {Graphs}},
	url = {http://ceur-ws.org/Vol-1875/paper20.pdf},
	language = {eng},
	urldate = {2025-11-19},
	publisher = {CEUR-WS.org},
	author = {Lisi, Francesca Alessandra and Stepanova, Daria},
	year = {2017},
}

@article{tang_exploring_2023,
	title = {Exploring {Research} on the {Construction} and {Application} of {Knowledge} {Graphs} for {Aircraft} {Fault} {Diagnosis}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/23/11/5295},
	doi = {10.3390/s23115295},
	abstract = {Fault diagnosis is crucial for repairing aircraft and ensuring their proper functioning. However, with the higher complexity of aircraft, some traditional diagnosis methods that rely on experience are becoming less effective. Therefore, this paper explores the construction and application of an aircraft fault knowledge graph to improve the efficiency of fault diagnosis for maintenance engineers. Firstly, this paper analyzes the knowledge elements required for aircraft fault diagnosis, and defines a schema layer of a fault knowledge graph. Secondly, with deep learning as the main method and heuristic rules as the auxiliary method, fault knowledge is extracted from structured and unstructured fault data, and a fault knowledge graph for a certain type of craft is constructed. Finally, a fault question-answering system based on a fault knowledge graph was developed, which can accurately answer questions from maintenance engineers. The practical implementation of our proposed methodology highlights how knowledge graphs provide an effective means of managing aircraft fault knowledge, ultimately assisting engineers in identifying fault roots accurately and quickly.},
	language = {en},
	number = {11},
	urldate = {2025-11-18},
	journal = {Sensors},
	author = {Tang, Xilang and Chi, Guo and Cui, Lijie and Ip, Andrew W. H. and Yung, Kai Leung and Xie, Xiaoyue},
	month = jan,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {aircraft fault diagnosis, deep learning, fault knowledge extraction, knowledge graph, question-answering system},
	pages = {5295},
}

@article{deng_research_2022,
	title = {Research on {Event} {Logic} {Knowledge} {Graph} {Construction} {Method} of {Robot} {Transmission} {System} {Fault} {Diagnosis}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9709362},
	doi = {10.1109/ACCESS.2022.3150409},
	abstract = {Knowledge graph technology has important guiding significance for efficient and orderly fault diagnosis of robot transmission system. Taking the historical robot maintenance logs of robot transmission system as the research object, a top-down fault diagnosis event logic knowledge graph construction method is proposed. Firstly, we define event arguments of fault phenomenon and fault cause events, define event argument classes and relation between classes, and construct an event logic knowledge ontology model. According to the event logic knowledge ontology, the fault diagnosis event argument entity and relation in the corpus are labeled, and an event logic knowledge extraction dataset is formed. Secondly, an event argument entity and relation joint extraction model is proposed. Using stacked bidirectional long short-term memory(BiLSTM) to obtain deep context features of text. As a supplement to stacked BiLSTM, self-attention mechanism extracts character dependency features from multiple subspaces, and uses conditional random field(CRF) to realize entity recognition. The character dependency features are mapped to the entity label weight embedding, and spliced with deep context features to extract relations. Bidirectional graph convolutional network(BiGCN) is introduced for relation inference, graph convolution features are used to update deep context features to perform joint extraction in the second phase. Experimental results show that this method can improve the effect of event argument entity and relation joint extraction and is better than other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed, which provides decision support for autonomous fault diagnosis of robot transmission system.},
	urldate = {2025-11-18},
	journal = {IEEE Access},
	author = {Deng, Jianfeng and Wang, Tao and Wang, Zhuowei and Zhou, Jiale and Cheng, Lianglun},
	year = {2022},
	keywords = {BiGCN, Event logic knowledge graph, Fault diagnosis, Feature extraction, Knowledge engineering, Ontologies, Robots, Semantics, Service robots, event argument knowledge extraction, fault diagnosis ontology, self-attention, stacked BiLSTM},
	pages = {17656--17673},
}

@article{han_construction_2022,
	title = {Construction and {Evolution} of {Fault} {Diagnosis} {Knowledge} {Graph} in {Industrial} {Process}},
	volume = {71},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/document/9863829},
	doi = {10.1109/TIM.2022.3200429},
	abstract = {The steel industry production line is complicated and contains a substantial amount of equipment, leading to serious problems in fault diagnosis (FD) such as wrong inspection strategies, fault location, and maintenance. To realize accurate and efficient equipment FD, this article proposes a steel production line equipment FD knowledge graph (SPLEFD-KG) based on a novel relation-oriented model with global context information for jointly extracting overlapping relations and entities (ROMGCJE). A low-level self-learning SPLEFD-KG is first constructed using the triples extracted by ROMGCJE. However, this low-level SPLEFD-KG is incomplete, and only contains sparse paths for fault reasoning. To overcome this problem, a reinforcement learning (RL) framework is applied to mine hidden semantic knowledge to complete the missing relation. Besides, the graph neural networks (Graph-NNs) are introduced to compute the embedding vector of new entities outside of the SPLEFD-KG for continuously completing missing entities. Finally, the low-level self-learning SPLEFD-KG evolves to one high-level SPLEFD-KG, which can provide information-rich and accurate fault-related knowledge. Extensive experiments conducted on the steel production line equipment failure (SPLEF) dataset indicate that the novel SPLEFD-KG significantly improves FD results and provides effective maintenance programs.},
	urldate = {2025-11-18},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Han, Huihui and Wang, Jian and Wang, Xiaowen and Chen, Sen},
	year = {2022},
	keywords = {Context modeling, Data mining, Entity and relation joint extraction, Maintenance engineering, Semantics, Steel, Steel industry, Task analysis, fault diagnosis (FD), graph neural networks (Graph-NNs), knowledge graph (KG), reinforcement learning (RL)},
	pages = {1--12},
}

@article{paulheim_knowledge_2016,
	title = {Knowledge graph refinement: {A} survey of approaches and evaluation methods},
	volume = {8},
	issn = {1570-0844},
	shorttitle = {Knowledge graph refinement},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.3233/SW-160218},
	abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term ‚ÄúKnowledge Graph‚Äù in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	language = {EN},
	number = {3},
	urldate = {2025-11-18},
	journal = {Semantic Web},
	author = {Paulheim, Heiko},
	month = dec,
	year = {2016},
	note = {Publisher: SAGE Publications},
	pages = {489--508},
}

@article{ye_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Graph} {Neural} {Networks} for {Knowledge} {Graphs}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/9831453},
	doi = {10.1109/ACCESS.2022.3191784},
	abstract = {The Knowledge graph, a multi-relational graph that represents rich factual information among entities of diverse classifications, has gradually become one of the critical tools for knowledge management. However, the existing knowledge graph still has some problems which form hot research topics in recent years. Numerous methods have been proposed based on various representation techniques. Graph Neural Network, a framework that uses deep learning to process graph-structured data directly, has significantly advanced the state-of-the-art in the past few years. This study firstly is aimed at providing a broad, complete as well as comprehensive overview of GNN-based technologies for solving four different KG tasks, including link prediction, knowledge graph alignment, knowledge graph reasoning, and node classification. Further, we also investigated the related artificial intelligence applications of knowledge graphs based on advanced GNN methods, such as recommender systems, question answering, and drug-drug interaction. This review will provide new insights for further study of KG and GNN.},
	urldate = {2025-11-18},
	journal = {IEEE Access},
	author = {Ye, Zi and Kumar, Yogan Jaya and Sing, Goh Ong and Song, Fengyan and Wang, Junsong},
	year = {2022},
	keywords = {Cognition, Deep learning, Graph neural networks, Internet, Knowledge engineering, Predictive models, Task analysis, distributed embedding, graph neural network, knowledge graph, representation learning},
	pages = {75729--75741},
}

@article{wang_knowledge_2017,
	title = {Knowledge {Graph} {Embedding}: {A} {Survey} of {Approaches} and {Applications}},
	volume = {29},
	issn = {1558-2191},
	shorttitle = {Knowledge {Graph} {Embedding}},
	url = {https://ieeexplore.ieee.org/abstract/document/8047276},
	doi = {10.1109/TKDE.2017.2754499},
	abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
	number = {12},
	urldate = {2025-11-18},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
	month = dec,
	year = {2017},
	keywords = {Graphical models, Knowledge discovery, Market research, Matrix decomposition, Semantics, Statistical analysis, Statistical relational learning, Systematics, knowledge graph embedding, latent factor models, tensor/matrix factorization models},
	pages = {2724--2743},
}

@inproceedings{procko_graph_2024,
	title = {Graph {Retrieval}-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Graph {Retrieval}-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/abstract/document/10771030},
	doi = {10.1109/AIxSET62544.2024.00030},
	abstract = {Large Language Models (LLMs) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without retraining, are 1) prompt engineering and 2) Retrieval-Augmented Generation (RAG). RAG is a form of prompt engineering, insofar as relevant lexical snippets retrieved from RAG corpora are vectorized and aggregated with prompts. However, RAG documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pretraining a LLM on massive and general corpora is to engender a generally applicable model, RAG is not: it is a means of LLM optimization, and as such, RAG document selection must be precise, not general. For expert tasks, it is imperative that a RAG corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs (KGs) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a better understanding of this novel research area for future work.},
	urldate = {2025-11-18},
	booktitle = {2024 {Conference} on {AI}, {Science}, {Engineering}, and {Technology} ({AIxSET})},
	author = {Procko, Tyler Thomas and Ochoa, Omar},
	month = sep,
	year = {2024},
	keywords = {GPT, Knowledge graphs, LLM, Large language models, Noise, Noise measurement, Optimization, Prompt engineering, RAG, Reliability, Surveys, Training, Uncertainty, fine-tuning, knowledge graphs},
	pages = {166--169},
}

@inproceedings{dessi_cs-kg_2022,
	address = {Cham},
	title = {{CS}-{KG}: {A} {Large}-{Scale} {Knowledge} {Graph} of¬†{Research} {Entities} and¬†{Claims} in¬†{Computer} {Science}},
	isbn = {978-3-031-19433-7},
	shorttitle = {{CS}-{KG}},
	doi = {10.1007/978-3-031-19433-7_39},
	abstract = {In recent years, we saw the emergence of several approaches for producing machine-readable, semantically rich, interlinked description of the content of research publications, typically encoded as knowledge graphs. A common limitation of these solutions is that they address a low number of articles, either because they rely on human experts to summarize information from the literature or because they focus on specific research areas. In this paper, we introduce the Computer Science Knowledge Graph (CS-KG), a large-scale knowledge graph composed by over 350M RDF triples describing 41M statements from 6.7M articles about 10M entities linked by 179 semantic relations. It was automatically generated and will be periodically updated by applying an information extraction pipeline on a large repository of research papers. CS-KG is much larger than all comparable solutions and offers a very comprehensive representation of tasks, methods, materials, and metrics in Computer Science. It can support a variety of intelligent services, such as advanced literature search, document classification, article recommendation, trend forecasting, hypothesis generation, and many others. CS-KG was evaluated against a benchmark of manually annotated statements, yielding excellent results.},
	language = {en},
	booktitle = {The {Semantic} {Web} ‚Äì {ISWC} 2022},
	publisher = {Springer International Publishing},
	author = {Dess√≠, Danilo and Osborne, Francesco and Reforgiato Recupero, Diego and Buscaldi, Davide and Motta, Enrico},
	editor = {Sattler, Ulrike and Hogan, Aidan and Keet, Maria and Presutti, Valentina and Almeida, Jo√£o Paulo A. and Takeda, Hideaki and Monnin, Pierre and Pirr√≤, Giuseppe and d‚ÄôAmato, Claudia},
	year = {2022},
	keywords = {Artificial Intelligence, Information extraction, Knowledge graph, Natural language processing, Scholarly data, Semantic Web},
	pages = {678--696},
}

@inproceedings{jaradeh_open_2019,
	address = {New York, NY, USA},
	series = {K-{CAP} '19},
	title = {Open {Research} {Knowledge} {Graph}: {Next} {Generation} {Infrastructure} for {Semantic} {Scholarly} {Knowledge}},
	isbn = {978-1-4503-7008-0},
	shorttitle = {Open {Research} {Knowledge} {Graph}},
	url = {https://dl.acm.org/doi/10.1145/3360901.3364435},
	doi = {10.1145/3360901.3364435},
	abstract = {Despite improved digital access to scholarly knowledge in recent decades, scholarly communication remains exclusively document-based. In this form, scholarly knowledge is hard to process automatically. We present the first steps towards a knowledge graph based infrastructure that acquires scholarly knowledge in machine actionable form thus enabling new possibilities for scholarly knowledge curation, publication and processing. The primary contribution is to present, evaluate and discuss multi-modal scholarly knowledge acquisition, combining crowdsourced and automated techniques. We present the results of the first user evaluation of the infrastructure with the participants of a recent international conference. Results suggest that users were intrigued by the novelty of the proposed infrastructure and by the possibilities for innovative scholarly knowledge processing it could enable.},
	urldate = {2025-11-18},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Knowledge} {Capture}},
	publisher = {Association for Computing Machinery},
	author = {Jaradeh, Mohamad Yaser and Oelen, Allard and Farfar, Kheir Eddine and Prinz, Manuel and D'Souza, Jennifer and Kismih√≥k, G√°bor and Stocker, Markus and Auer, S√∂ren},
	month = sep,
	year = {2019},
	pages = {243--246},
}

@article{himmelstein_systematic_nodate,
	title = {Systematic integration of biomedical knowledge prioritizes drugs for repurposing},
	volume = {6},
	issn = {2050-084X},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC5640425/},
	doi = {10.7554/eLife.26726},
	abstract = {The ability to computationally predict whether a compound treats a disease would improve the economy and success rate of drug approval. This study describes Project Rephetio to systematically model drug efficacy based on 755 existing treatments. First, we constructed Hetionet (neo4j.het.io), an integrative network encoding knowledge from millions of biomedical studies. Hetionet v1.0 consists of 47,031 nodes of 11 types and 2,250,197 relationships of 24 types. Data were integrated from 29 public resources to connect compounds, diseases, genes, anatomies, pathways, biological processes, molecular functions, cellular components, pharmacologic classes, side effects, and symptoms. Next, we identified network patterns that distinguish treatments from non-treatments. Then, we predicted the probability of treatment for 209,168 compound‚Äìdisease pairs (het.io/repurpose). Our predictions validated on two external sets of treatment and provided pharmacological insights on epilepsy, suggesting they will help prioritize drug repurposing candidates. This study was entirely open and received realtime feedback from 40 community members., Of all the data in the world today, 90\% was created in the last two years. However, taking advantage of this data in order to advance our knowledge is restricted by how quickly we can access it and analyze it in a proper context., In biomedical research, data is largely fragmented and stored in databases that typically do not ‚Äútalk‚Äù to each other, thus hampering progress. One particular problem in medicine today is that the process of making a new therapeutic drug from scratch is incredibly expensive and inefficient, making it a risky business. Given the low success rate in drug discovery, there is an economic incentive in trying to repurpose an existing drug that has already been shown to be safe and effective towards a new disease or condition., Himmelstein et al. used a computational approach to analyze 50,000 data points ‚Äì including drugs, diseases, genes and symptoms ‚Äì from 19 different public databases. This approach made it possible to create more than two million relationships among the data points, which could be used to develop models that predict which drugs currently in use by doctors might be best suited to treat any of 136 common diseases. For example, Himmelstein et al. identified specific drugs currently used to treat depression and alcoholism that could be repurposed to treat smoking addition and epilepsy., These findings provide a new and powerful way to study drug repurposing. While this work was exclusively performed with public data, an expanded and potentially stronger set of predictions could be obtained if data owned by pharmaceutical companies were incorporated. Additional studies will be needed to test the predictions made by the models.},
	urldate = {2025-11-18},
	journal = {eLife},
	author = {Himmelstein, Daniel Scott and Lizee, Antoine and Hessler, Christine and Brueggeman, Leo and Chen, Sabrina L and Hadley, Dexter and Green, Ari and Khankhanian, Pouya and Baranzini, Sergio E},
	pmid = {28936969},
	pmcid = {PMC5640425},
	pages = {e26726},
}

@inproceedings{hubert_knowledge_2022,
	title = {Knowledge {Graph} {Embeddings} for {Link} {Prediction}: {Beware} of {Semantics}!},
	shorttitle = {Knowledge {Graph} {Embeddings} for {Link} {Prediction}},
	url = {https://www.semanticscholar.org/paper/Knowledge-Graph-Embeddings-for-Link-Prediction%3A-of-Hubert-Monnin/2ad7ba82386c46ff03e2ca2cfd3459bd5e5e3abc},
	abstract = {The task of predicting links in knowledge graphs (KGs) can be tackled using knowledge graph embedding models (KGEMs). Such models project entities and relations of a KG into a low-dimensional vector space that preserves as much as possible the properties of the graph. The performance of KGEMs for link prediction is traditionally assessed using rank-based metrics that evaluate the ability of models to give high scores to ground-truth entities. However, other scored entities are left unconsidered by these metrics. This constitutes a shortcoming in some application domains where it may be required to ensure consistency among the top-scored entities. To this aim, in this paper we propose to measure the ability of popular KGEMs to capture the semantic profile of relations. In particular, we use Sem@ ùêæ , a semantic-oriented metric that assesses whether top-scored entities are semantically valid. Our experiments show that agnostic KGEMs are actually able to learn the semantic profile of relations. This raises the opportunity of using Sem@ ùêæ as an additional training criterion.},
	urldate = {2025-10-21},
	author = {Hubert, Nicolas and Monnin, P. and Brun, A. and Monticolo, D.},
	year = {2022},
}

@book{hogan_knowledge_2022,
	address = {Cham},
	series = {Synthesis {Lectures} on {Data}, {Semantics}, and {Knowledge}},
	title = {Knowledge {Graphs}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-00790-3 978-3-031-01918-0},
	url = {https://link.springer.com/10.1007/978-3-031-01918-0},
	language = {en},
	urldate = {2025-04-17},
	publisher = {Springer International Publishing},
	author = {Hogan, Aidan and Gutierrez, Claudio and Cochez, Michael and Melo, Gerard De and Kirrane, Sabrina and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Schmelzeisen, Lukas and Staab, Steffen and Blomqvist, Eva and d‚ÄôAmato, Claudia and Gayo, Jos√© Emilio Labra and Neumaier, Sebastian and Rula, Anisa and Sequeda, Juan and Zimmermann, Antoine},
	year = {2022},
	doi = {10.1007/978-3-031-01918-0},
}

@article{pan_approximating_nodate,
	title = {Approximating {OWL}-{DL} {Ontologies}},
	abstract = {EfÔ¨Åcient query answering over ontologies is one of the most useful and important services to support Semantic Web applications. Approximation has been identiÔ¨Åed as a potential way to reduce the complexity of query answering over OWL DL ontologies. Existing approaches are mainly based on syntactic approximation of ontological axioms and queries. In this paper, we propose to recast the idea of knowledge compilation into approximating OWL DL ontologies with DL-Lite ontologies, against which query answering has only polynomial data complexity. We identify a useful category of queries for which our approach guarantees also completeness. Furthermore, this paper reports on the implementation of our approach in the ONTOSEARCH2 system and preliminary, but encouraging, benchmark results which compare ONTOSEARCH2‚Äôs response times on a number of queries with those of existing ontology reasoning systems.},
	language = {en},
	author = {Pan, Jeff Z},
}

@incollection{hutchison_owl_2013,
	address = {Berlin, Heidelberg},
	title = {From {OWL} to {DL} ‚àí {Lite} through {Efficient} {Ontology} {Approximation}},
	volume = {7994},
	isbn = {978-3-642-39665-6 978-3-642-39666-3},
	url = {http://link.springer.com/10.1007/978-3-642-39666-3_20},
	language = {en},
	urldate = {2025-10-01},
	booktitle = {Web {Reasoning} and {Rule} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Console, Marco and Santarelli, Valerio and Savo, Domenico Fabio},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Faber, Wolfgang and Lembo, Domenico},
	year = {2013},
	doi = {10.1007/978-3-642-39666-3_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {229--234},
}

@article{calvanese_tractable_2007,
	title = {Tractable {Reasoning} and {Efficient} {Query} {Answering} in {Description} {Logics}: {The} {DL}-{Lite} {Family}},
	volume = {39},
	issn = {1573-0670},
	shorttitle = {Tractable {Reasoning} and {Efficient} {Query} {Answering} in {Description} {Logics}},
	url = {https://doi.org/10.1007/s10817-007-9078-x},
	doi = {10.1007/s10817-007-9078-x},
	abstract = {We propose a new family of description logics (DLs), called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, unions of conjunctive queries) over the instance level (ABox) of the DL knowledge base. We show that, for the DLs of the DL-Lite¬†family, the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is LogSpace in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial-time data complexity for query answering over DL knowledge bases. Notably our logics allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current database management systems. Since even slight extensions to the logics of the DL-Lite¬†family make query answering at least NLogSpace in data complexity, thus ruling out the possibility of using on-the-shelf relational technology for query processing, we can conclude that the logics of the DL-Lite¬†family are the maximal DLs supporting efficient query answering over large amounts of instances.},
	language = {en},
	number = {3},
	urldate = {2025-10-01},
	journal = {Journal of Automated Reasoning},
	author = {Calvanese, Diego and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Rosati, Riccardo},
	month = oct,
	year = {2007},
	keywords = {DL-Lite, Description logics, Ontology languages, Query answering},
	pages = {385--429},
}

@article{artale_dl-lite_2009,
	title = {The {DL}-{Lite} {Family} and {Relations}},
	volume = {36},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10623},
	doi = {10.1613/jair.2820},
	abstract = {The recently introduced series of description logics under the common moniker ‚ÄòDLLite‚Äô has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent conceptual modeling formalisms, on the other. The main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original DL-Lite logics along Ô¨Åve axes: by (i) adding the Boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reÔ¨Çexivity, irreÔ¨Çexivity and transitivity constraints, and (v) adopting or dropping the unique name assumption. We analyze the combined complexity of satisÔ¨Åability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries. Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable Ô¨Årst-order logic, which provides useful insights into their properties and, in particular, computational behavior.},
	language = {en},
	urldate = {2025-10-01},
	journal = {Journal of Artificial Intelligence Research},
	author = {Artale, A. and Calvanese, D. and Kontchakov, R. and Zakharyaschev, M.},
	month = oct,
	year = {2009},
	pages = {1--69},
}

@inproceedings{roussey_catalogue_2009,
	address = {Redondo Beach California USA},
	title = {A catalogue of {OWL} ontology antipatterns},
	isbn = {978-1-60558-658-8},
	url = {https://dl.acm.org/doi/10.1145/1597735.1597784},
	doi = {10.1145/1597735.1597784},
	abstract = {Debugging inconsistent OWL ontologies is a timeconsuming task. Debugging services included in existing ontology engineering tools are still far from providing adequate support to ontology developers and domain experts for this task, due to their lack of efficiency or precision when explaining the main causes for inconsistencies. We present a catalogue of common antipatterns found in inconsistent ontologies that can be used in combination with these tools to make this task more effective.},
	language = {en},
	urldate = {2025-10-01},
	booktitle = {Proceedings of the fifth international conference on {Knowledge} capture},
	publisher = {ACM},
	author = {Roussey, Catherine and Corcho, Oscar and Vilches-Bl√°zquez, Luis Manuel},
	month = sep,
	year = {2009},
	pages = {205--206},
}

@inproceedings{de_groot_analysing_2021,
	address = {Cham},
	title = {Analysing {Large} {Inconsistent} {Knowledge} {Graphs} {Using} {Anti}-patterns},
	isbn = {978-3-030-77385-4},
	doi = {10.1007/978-3-030-77385-4_3},
	abstract = {A number of Knowledge Graphs (KGs) on the Web of Data contain contradicting statements, and therefore are logically inconsistent. This makes reasoning limited and the knowledge formally useless. Understanding how these contradictions are formed, how often they occur, and how they vary between different KGs is essential for fixing such contradictions, or developing better tools that handle inconsistent KGs. Methods exist to explain a single contradiction, by finding the minimal set of axioms sufficient to produce it, a process known as justification retrieval. In large KGs, these justifications can be frequent and might redundantly refer to the same type of modelling mistake. Furthermore, these justifications are ‚Äìby definition‚Äì domain dependent, and hence difficult to interpret or compare. This paper uses the notion of anti-pattern for generalising these justifications, and presents an approach for detecting almost all anti-patterns from any inconsistent KG. Experiments on KGs of over 28 billion triples show the scalability of this approach, and the benefits of anti-patterns for analysing and comparing logical errors between different KGs.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer International Publishing},
	author = {de Groot, Thomas and Raad, Joe and Schlobach, Stefan},
	editor = {Verborgh, Ruben and Hose, Katja and Paulheim, Heiko and Champin, Pierre-Antoine and Maleshkova, Maria and Corcho, Oscar and Ristoski, Petar and Alam, Mehwish},
	year = {2021},
	keywords = {Inconsistency, Linked open data, Reasoning},
	pages = {40--56},
}

@misc{ott_safran_2021,
	title = {{SAFRAN}: {An} interpretable, rule-based link prediction method outperforming embedding models},
	shorttitle = {{SAFRAN}},
	url = {http://arxiv.org/abs/2109.08002},
	doi = {10.48550/arXiv.2109.08002},
	abstract = {Neural embedding-based machine learning models have shown promise for predicting novel links in knowledge graphs. Unfortunately, their practical utility is diminished by their lack of interpretability. Recently, the fully interpretable, rule-based algorithm AnyBURL yielded highly competitive results on many general-purpose link prediction benchmarks. However, current approaches for aggregating predictions made by multiple rules are affected by redundancies. We improve upon AnyBURL by introducing the SAFRAN rule application framework, which uses a novel aggregation approach called Non-redundant Noisy-OR that detects and clusters redundant rules prior to aggregation. SAFRAN yields new state-of-the-art results for fully interpretable link prediction on the established general-purpose benchmarks FB15K-237, WN18RR and YAGO3-10. Furthermore, it exceeds the results of multiple established embedding-based algorithms on FB15K-237 and WN18RR and narrows the gap between rule-based and embedding-based algorithms on YAGO3-10.},
	urldate = {2025-09-30},
	publisher = {arXiv},
	author = {Ott, Simon and Meilicke, Christian and Samwald, Matthias},
	month = sep,
	year = {2021},
	note = {arXiv:2109.08002 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{pirro_relatedness_2020,
	title = {Relatedness and {TBox}-{Driven} {Rule} {Learning} in {Large} {Knowledge} {Bases}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5690},
	doi = {10.1609/aaai.v34i03.5690},
	abstract = {We present RARL, an approach to discover rules of the form body ‚áí head in large knowledge bases (KBs) that typically include a set of terminological facts (TBox) and a set of TBox-compliant assertional facts (ABox). RARL's main intuition is to learn rules by leveraging TBox-information and the semantic relatedness between the predicate(s) in the atoms of the body and the predicate in the head. RARL uses an efficient relatedness-driven TBox traversal algorithm, which given an input rule head, generates the set of most semantically related candidate rule bodies. Then, rule confidence is computed in the ABox based on a set of positive and negative examples. Decoupling candidate generation and rule quality assessment offers greater flexibility than previous work.},
	language = {en},
	number = {03},
	urldate = {2025-09-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pirr√≤, Giuseppe},
	month = apr,
	year = {2020},
	pages = {2975--2982},
}

@incollection{harth_fast_2020,
	address = {Cham},
	title = {Fast and {Exact} {Rule} {Mining} with {AMIE} 3},
	volume = {12123},
	isbn = {978-3-030-49460-5 978-3-030-49461-2},
	url = {https://link.springer.com/10.1007/978-3-030-49461-2_3},
	abstract = {Given a knowledge base (KB), rule mining Ô¨Ånds rules such as ‚ÄúIf two people are married, then they live (most likely) in the same place‚Äù. Due to the exponential search space, rule mining approaches still have diÔ¨Éculties to scale to today‚Äôs large KBs. In this paper, we present AMIE 3, a system that employs a number of sophisticated pruning strategies and optimizations. This allows the system to mine rules on large KBs in a matter of minutes. Most importantly, we do not have to resort to approximations or sampling, but are able to compute the exact conÔ¨Ådence and support of each rule. Our experiments on DBpedia, YAGO, and Wikidata show that AMIE 3 beats the state of the art by a factor of more than 15 in terms of runtime.},
	language = {en},
	urldate = {2025-09-29},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer International Publishing},
	author = {Lajus, Jonathan and Gal√°rraga, Luis and Suchanek, Fabian},
	editor = {Harth, Andreas and Kirrane, Sabrina and Ngonga Ngomo, Axel-Cyrille and Paulheim, Heiko and Rula, Anisa and Gentile, Anna Lisa and Haase, Peter and Cochez, Michael},
	year = {2020},
	doi = {10.1007/978-3-030-49461-2_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {36--52},
}

@inproceedings{tran_fast_2020,
	address = {New York, NY, USA},
	series = {{WWW} '20},
	title = {Fast {Computation} of {Explanations} for {Inconsistency} in {Large}-{Scale} {Knowledge} {Graphs}},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380014},
	doi = {10.1145/3366423.3380014},
	abstract = {Knowledge graphs (KGs) are essential resources for many applications including Web search and question answering. As KGs are often automatically constructed, they may contain incorrect facts. Detecting them is a crucial, yet extremely expensive task. Prominent solutions detect and explain inconsistency in KGs with respect to accompanying ontologies that describe the KG domain of interest. Compared to machine learning methods they are more reliable and human-interpretable but scale poorly on large KGs. In this paper, we present a novel approach to dramatically speed up the process of detecting and explaining inconsistency in large KGs by exploiting KG abstractions that capture prominent data patterns. Though much smaller, KG abstractions preserve inconsistency and their explanations. Our experiments with large KGs (e.g., DBpedia and Yago) demonstrate the feasibility of our approach and show that it significantly outperforms the popular baseline.},
	urldate = {2025-09-18},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {Association for Computing Machinery},
	author = {Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria and Kharlamov, Evgeny and Str√∂tgen, Jannik},
	month = apr,
	year = {2020},
	pages = {2613--2619},
}

@misc{nentidis_dealing_2025,
	title = {Dealing with {Inconsistency} for {Reasoning} over {Knowledge} {Graphs}: {A} {Survey}},
	shorttitle = {Dealing with {Inconsistency} for {Reasoning} over {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2502.19023},
	doi = {10.48550/arXiv.2502.19023},
	abstract = {In Knowledge Graphs (KGs), where the schema of the data is usually defined by particular ontologies, reasoning is a necessity to perform a range of tasks, such as retrieval of information, question answering, and the derivation of new knowledge. However, information to populate KGs is often extracted (semi-) automatically from natural language resources, or by integrating datasets that follow different semantic schemas, resulting in KG inconsistency. This, however, hinders the process of reasoning. In this survey, we focus on how to perform reasoning on inconsistent KGs, by analyzing the state of the art towards three complementary directions: a) the detection of the parts of the KG that cause the inconsistency, b) the fixing of an inconsistent KG to render it consistent, and c) the inconsistency-tolerant reasoning. We discuss existing work from a range of relevant fields focusing on how, and in which cases they are related to the above directions. We also highlight persisting challenges and future directions.},
	language = {en},
	urldate = {2025-09-18},
	publisher = {arXiv},
	author = {Nentidis, Anastasios and Akasiadis, Charilaos and Charalambidis, Angelos and Artikis, Alexander},
	month = feb,
	year = {2025},
	note = {arXiv:2502.19023 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{xiong_deeppath_2017,
	address = {Copenhagen, Denmark},
	title = {{DeepPath}: {A} {Reinforcement} {Learning} {Method} for {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{DeepPath}},
	url = {https://aclanthology.org/D17-1060/},
	doi = {10.18653/v1/D17-1060},
	abstract = {We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.},
	urldate = {2025-09-15},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
	editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
	month = sep,
	year = {2017},
	pages = {564--573},
}

@inproceedings{hubert_treat_2024,
	address = {Cham},
	title = {Treat {Different} {Negatives} {Differently}: {Enriching} {Loss} {Functions} with¬†{Domain} and¬†{Range} {Constraints} for¬†{Link} {Prediction}},
	isbn = {978-3-031-60626-7},
	shorttitle = {Treat {Different} {Negatives} {Differently}},
	doi = {10.1007/978-3-031-60626-7_2},
	abstract = {Knowledge graph embedding models (KGEMs) are used for various tasks related to knowledge graphs (KGs), including link prediction. They are trained with loss functions that consider batches of true and false triples. However, different kinds of false triples exist and recent works suggest that they should not be valued equally, leading to specific negative sampling procedures. In line with this recent assumption, we posit that negative triples that are semantically valid w.r.t. signatures of relations (domain and range) are high-quality negatives. Hence, we enrich the three main loss functions for link prediction such that all kinds of negatives are sampled but treated differently based on their semantic validity. In an extensive and controlled experimental setting, we show that the proposed loss functions systematically provide satisfying results which demonstrates both the generality and superiority of our proposed approach. In fact, the proposed loss functions (1) lead to better MRR and Hits@10 values, and (2) drive KGEMs towards better semantic correctness as measured by the Sem@K metric. This highlights that relation signatures globally improve KGEMs, and thus should be incorporated into loss functions. Domains and ranges of relations being largely available in schema-defined KGs, this makes our approach both beneficial and widely usable in practice.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer Nature Switzerland},
	author = {Hubert, Nicolas and Monnin, Pierre and Brun, Armelle and Monticolo, Davy},
	editor = {Mero√±o Pe√±uela, Albert and Dimou, Anastasia and Troncy, Rapha√´l and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	year = {2024},
	keywords = {Knowledge Graph Embeddings, Link Prediction, Loss Functions, Schema-based Learning},
	pages = {22--40},
}

@inproceedings{suchanek_yago_2024,
	address = {Washington DC USA},
	title = {{YAGO} 4.5: {A} {Large} and {Clean} {Knowledge} {Base} with a {Rich} {Taxonomy}},
	isbn = {979-8-4007-0431-4},
	shorttitle = {{YAGO} 4.5},
	url = {https://dl.acm.org/doi/10.1145/3626772.3657876},
	doi = {10.1145/3626772.3657876},
	abstract = {Knowledge Bases (KBs) find applications in many knowledgeintensive tasks and, most notably, in information retrieval. Wikidata is one of the largest public general-purpose KBs. Yet, its collaborative nature has led to a convoluted schema and taxonomy. The YAGO 4 KB cleaned up the taxonomy by incorporating the ontology of Schema.org, resulting in a cleaner structure amenable to automated reasoning. However, it also cut away large parts of the Wikidata taxonomy, which is essential for information retrieval. In this paper, we extend YAGO 4 with a large part of the Wikidata taxonomy ‚Äì while respecting logical constraints and the distinction between classes and instances. This yields YAGO 4.5, a new, logically consistent version of YAGO that adds a rich layer of informative classes. An intrinsic and an extrinsic evaluation show the value of the new resource.},
	language = {en},
	urldate = {2025-08-27},
	booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Suchanek, Fabian M. and Alam, Mehwish and Bonald, Thomas and Chen, Lihu and Paris, Pierre-Henri and Soria, Jules},
	month = jul,
	year = {2024},
	pages = {131--140},
}

@article{wang_schema-aware_nodate,
	title = {Schema-aware {Iterative} {Completion} for {Knowledge} {Graphs} {Revisited}},
	abstract = {Recent success of knowledge graph (KG) has spurred widespread interests in methods for the problem of Knowledge Graph Completion (KGC). However, efforts to understand the quality of the candidate triples from these methods, in particular from the schema aspect, have been limited. In fact, most existing Knowledge Graph completion methods do not guarantee that the expanded Knowledge Graphs are consistent with the schema of the initial Knowledge Graph. Hence, schema-aware KGC seems to be way to go.},
	language = {en},
	author = {Wang, Fangrong and Bundy, Alan and Li, Xue and Nuamah, Kwabena and Xu, Lei and Mauceri, Stefano and Pan, Jeff Z},
}

@article{meznar_ontology_2022,
	title = {Ontology {Completion} with {Graph}-{Based} {Machine} {Learning}: {A} {Comprehensive} {Evaluation}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	shorttitle = {Ontology {Completion} with {Graph}-{Based} {Machine} {Learning}},
	url = {https://www.mdpi.com/2504-4990/4/4/56},
	doi = {10.3390/make4040056},
	abstract = {Increasing quantities of semantic resources offer a wealth of human knowledge, but their growth also increases the probability of wrong knowledge base entries. The development of approaches that identify potentially spurious parts of a given knowledge base is therefore highly relevant. We propose an approach for ontology completion that transforms an ontology into a graph and recommends missing edges using structure-only link analysis methods. By systematically evaluating thirteen methods (some for knowledge graphs) on eight different semantic resources, including Gene Ontology, Food Ontology, Marine Ontology, and similar ontologies, we demonstrate that a structure-only link analysis can offer a scalable and computationally efficient ontology completion approach for a subset of analyzed data sets. To the best of our knowledge, this is currently the most extensive systematic study of the applicability of different types of link analysis methods across semantic resources from different domains. It demonstrates that by considering symbolic node embeddings, explanations of the predictions (links) can be obtained, making this branch of methods potentially more valuable than black-box methods.},
	language = {en},
	number = {4},
	urldate = {2025-08-20},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Me≈ænar, Sebastian and Bevec, Matej and Lavraƒç, Nada and ≈†krlj, Bla≈æ},
	month = dec,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {embedding, explainability, link prediction, machine learning, ontology completion},
	pages = {1107--1123},
}

@article{wiharja_schema_2020,
	title = {Schema aware iterative {Knowledge} {Graph} completion},
	volume = {65},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826820300494},
	doi = {10.1016/j.websem.2020.100616},
	abstract = {Recent success of Knowledge Graph has spurred widespread interests in methods for the problem of Knowledge Graph completion. However, efforts to understand the quality of the candidate triples from these methods, in particular from the schema aspect, have been limited. Indeed, most existing Knowledge Graph completion methods do not guarantee that the expanded Knowledge Graphs are consistent with the ontological schema of the initial Knowledge Graph. In this work, we challenge the silver standard method, by proposing the notion of schema-correctness. A fundamental challenge is how to make use of different types of Knowledge Graph completion methods together to improve the production of schema-correct triples. To address this, we analyse the characteristics of different methods and propose a schema aware iterative approach to Knowledge Graph completion. Our main findings are: (i) Some popular Knowledge Graph completion methods have surprisingly low schema-correctness ratio; (ii) Different types of Knowledge Graph completion methods can work with each other to help overcame individual limitations; (iii) Some iterative sequential combinations of Knowledge Graph completion methods have significantly better schema-correctness and coverage ratios than other combinations; (iv) All the MapReduce based iterative methods outperform involved single-pass methods significantly over the tested Knowledge Graphs in terms of productivity of schema-correct triples. Our findings and infrastructure can help further work on evaluating Knowledge Graph completion methods, more fine-grained approaches for schema aware iterative knowledge graph completion, as well as new approximate reasoning approaches based Knowledge Graph completion methods.},
	urldate = {2025-08-20},
	journal = {Journal of Web Semantics},
	author = {Wiharja, Kemas and Pan, Jeff Z. and Kollingbaum, Martin J. and Deng, Yu},
	month = dec,
	year = {2020},
	keywords = {Approximate reasoning, Correctness and coverage, Knowledge Graph completion, Knowledge Graph reasoning, SHACL constraint, Schema aware},
	pages = {100616},
}

@article{wu_rule_2023,
	title = {Rule {Learning} over {Knowledge} {Graphs}: {A} {Review}},
	volume = {1},
	copyright = {Creative Commons Attribution 4.0 International license, info:eu-repo/semantics/openAccess},
	issn = {2942-7517},
	shorttitle = {Rule {Learning} over {Knowledge} {Graphs}},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/TGDK.1.1.7},
	doi = {10.4230/TGDK.1.1.7},
	abstract = {Compared to black-box neural networks, logic rules express explicit knowledge, can provide human-understandable explanations for reasoning processes, and have found their wide application in knowledge graphs and other downstream tasks. As extracting rules manually from large knowledge graphs is labour-intensive and often infeasible, automated rule learning has recently attracted significant interest, and a number of approaches to rule learning for knowledge graphs have been proposed. This survey aims to provide a review of approaches and a classification of state-of-the-art systems for learning first-order logic rules over knowledge graphs. A comparative analysis of various approaches to rule learning is conducted based on rule language biases, underlying methods, and evaluation metrics. The approaches we consider include inductive logic programming (ILP)-based, statistical path generalisation, and neuro-symbolic methods. Moreover, we highlight important and promising application scenarios of rule learning, such as rule-based knowledge graph completion, fact checking, and applications in other research areas.},
	language = {en},
	number = {1},
	urldate = {2025-08-18},
	journal = {Transactions on Graph Data and Knowledge (TGDK)},
	author = {Wu, Hong and Wang, Zhe and Wang, Kewen and Omran, Pouya Ghiasnezhad and Li, Jiangmeng},
	editor = {Hogan, Aidan and Horrocks, Ian and Hotho, Andreas and Kagal, Lalana},
	year = {2023},
	note = {Artwork Size: 23 pages, 1239675 bytes
Medium: application/pdf
Publisher: Schloss Dagstuhl ‚Äì Leibniz-Zentrum f√ºr Informatik},
	keywords = {Computing methodologies ‚Üí Knowledge representation and reasoning, Information systems ‚Üí Data mining, Knowledge graphs, Link prediction, Rule learning},
	pages = {7:1--7:23},
}

@inproceedings{wu_learning_2022,
	address = {Haifa, Israel},
	title = {Learning {Typed} {Rules} over {Knowledge} {Graphs}},
	isbn = {978-1-956792-01-0},
	url = {https://proceedings.kr.org/2022/51},
	doi = {10.24963/kr.2022/51},
	abstract = {Rule learning from large datasets has regained extensive interest as rules are useful for developing explainable approaches to many applications in knowledge graphs. However, existing methods for rule learning are still limited in terms of rule expressivity and rule quality. This paper presents a new method for learning typed rules by employing type information. Our experimental evaluation shows the superiority of our system compared to state-of-the-art rule learners. In particular, we demonstrate the usefulness of typed rules in reasoning for link prediction.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the {Nineteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wu, Hong and Wang, Zhe and Wang, Kewen and Shen, Yi-Dong},
	month = jul,
	year = {2022},
	pages = {494--503},
}

@inproceedings{ott_rule-based_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {Rule-based {Knowledge} {Graph} {Completion} with {Canonical} {Models}},
	isbn = {979-8-4007-0124-5},
	url = {https://dl.acm.org/doi/10.1145/3583780.3615042},
	doi = {10.1145/3583780.3615042},
	abstract = {Rule-based approaches have proven to be an efficient and explainable method for knowledge base completion. Their predictive quality is on par with classic knowledge graph embedding models such as TransE or ComplEx, however, they cannot achieve the results of neural models proposed recently. The performance of a rule-based approach depends crucially on the solution of the rule aggregation problem, which is concerned with the computation of a score for a prediction that is generated by several rules. Within this paper, we propose a supervised approach to learn a reweighted confidence value for each rule to get an optimal explanation for the training set given a specific aggregation function. In particular, we apply our approach to two aggregation functions: We learn weights for a noisy-or multiplication and apply logistic regression, which computes the score of a prediction as a sum of these weights. Due to the simplicity of both models the final score is fully explainable. Our experimental results show that we can significantly improve the predictive quality of a rule-based approach. We compare our method with current state-of-the-art latent models that lack explainability, and achieve promising results.},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ott, Simon and Betz, Patrick and Stepanova, Daria and Gad-Elrab, Mohamed H. and Meilicke, Christian and Stuckenschmidt, Heiner},
	month = oct,
	year = {2023},
	pages = {1971--1981},
}

@inproceedings{tran_towards_2017,
	address = {Cham},
	title = {Towards {Nonmonotonic} {Relational} {Learning} from {Knowledge} {Graphs}},
	isbn = {978-3-319-63342-8},
	doi = {10.1007/978-3-319-63342-8_8},
	abstract = {Recent advances in information extraction have led to the so-called knowledge graphs (KGs), i.e., huge collections of relational factual knowledge. Since KGs are automatically constructed, they are inherently incomplete, thus naturally treated under the Open World Assumption (OWA). Rule mining techniques have been exploited to support the crucial task of KG completion. However, these techniques can mine Horn rules, which are insufficiently expressive to capture exceptions, and might thus make incorrect predictions on missing links. Recently, a rule-based method for filling in this gap was proposed which, however, applies to a flattened representation of a KG with only unary facts. In this work we make the first steps towards extending this approach to KGs in their original relational form, and provide preliminary evaluation results on real-world KGs, which demonstrate the effectiveness of our method.},
	language = {en},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Springer International Publishing},
	author = {Tran, Hai Dang and Stepanova, Daria and Gad-Elrab, Mohamed H. and Lisi, Francesca A. and Weikum, Gerhard},
	editor = {Cussens, James and Russo, Alessandra},
	year = {2017},
	keywords = {Horn Rules, Nominal Exclusion, Nonmonotonic Rule, Relational Association Rule Mining, Theory Revision},
	pages = {94--107},
}

@inproceedings{safavi_evaluating_2020,
	address = {Online},
	title = {Evaluating the {Calibration} of {Knowledge} {Graph} {Embeddings} for {Trustworthy} {Link} {Prediction}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.667},
	doi = {10.18653/v1/2020.emnlp-main.667},
	abstract = {Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output conÔ¨Ådence scores that reÔ¨Çect the expected correctness of predicted knowledge graph triples. We Ô¨Årst conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner‚Äôs perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.},
	language = {en},
	urldate = {2025-08-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Safavi, Tara and Koutra, Danai and Meij, Edgar},
	year = {2020},
	pages = {8308--8321},
}

@inproceedings{jain_improving_2021,
	address = {Cham},
	title = {Improving {Knowledge} {Graph} {Embeddings} with {Ontological} {Reasoning}},
	isbn = {978-3-030-88361-4},
	doi = {10.1007/978-3-030-88361-4_24},
	abstract = {Knowledge graph (KG) embedding models have emerged as powerful means for KG completion. To learn the representation of KGs, entities and relations are projected in a low-dimensional vector space so that not only existing triples in the KG are preserved but also new triples can be predicted. Embedding models might learn a good representation of the input KG, but due to the nature of machine learning approaches, they often lose the semantics of entities and relations, which might lead to nonsensical predictions. To address this issue we propose to improve the accuracy of embeddings using ontological reasoning. More specifically, we present a novel iterative approach ReasonKGE that identifies dynamically via symbolic reasoning inconsistent predictions produced by a given embedding model and feeds them as negative samples for retraining this model. In order to address the scalability problem that arises when integrating ontological reasoning into the training process, we propose an advanced technique to generalize the inconsistent predictions to other semantically similar negative samples during retraining. Experimental results demonstrate the improvements in accuracy of facts produced by our method compared to the state-of-the-art.},
	language = {en},
	booktitle = {The {Semantic} {Web} ‚Äì {ISWC} 2021},
	publisher = {Springer International Publishing},
	author = {Jain, Nitisha and Tran, Trung-Kien and Gad-Elrab, Mohamed H. and Stepanova, Daria},
	editor = {Hotho, Andreas and Blomqvist, Eva and Dietze, Stefan and Fokoue, Achille and Ding, Ying and Barnaghi, Payam and Haller, Armin and Dragoni, Mauro and Alani, Harith},
	year = {2021},
	pages = {410--426},
}

@inproceedings{hao_universal_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Universal {Representation} {Learning} of {Knowledge} {Bases} by {Jointly} {Embedding} {Instances} and {Ontological} {Concepts}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330838},
	doi = {10.1145/3292500.3330838},
	abstract = {Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding ontological concepts connected via a (small) set of cross-view links. Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology-view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance.},
	urldate = {2025-07-31},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun, Yizhou and Wang, Wei},
	month = jul,
	year = {2019},
	pages = {1709--1719},
}

@inproceedings{wiharja_more_2018,
	address = {Cham},
	title = {More {Is} {Better}: {Sequential} {Combinations} of {Knowledge} {Graph} {Embedding} {Approaches}},
	isbn = {978-3-030-04284-4},
	shorttitle = {More {Is} {Better}},
	doi = {10.1007/978-3-030-04284-4_2},
	abstract = {Constructing and maintaining large-scale good quality knowledge graphs present many challenges. Knowledge graph completion has been regarded a promising direction in the knowledge graph community. The majority of current work for knowledge graph completion approaches do not take the schema of a target knowledge graph as input. As a result, the triples generated by these approaches are not necessarily consistent with the schema of the target knowledge graph. This paper proposes to improve the correctness of knowledge graph completion based on Schema Aware Triple Classification (SATC), which enables sequential combinations of knowledge graph embedding approaches. Extensive experiments show that our proposed approaches can significantly improve the correctness of the new triples produced by knowledge graph embedding methods.},
	language = {en},
	booktitle = {Semantic {Technology}},
	publisher = {Springer International Publishing},
	author = {Wiharja, Kemas and Pan, Jeff Z. and Kollingbaum, Martin and Deng, Yu},
	editor = {Ichise, Ryutaro and Lecue, Freddy and Kawamura, Takahiro and Zhao, Dongyan and Muggleton, Stephen and Kozaki, Kouji},
	year = {2018},
	keywords = {Approximate reasoning, Artificial Intelligence, Embedding, Knowledge graph, Knowledge representation and reasoning, Schema aware triple classification},
	pages = {19--35},
}

@misc{suresh_hybrid_2020,
	title = {A {Hybrid} {Model} for {Learning} {Embeddings} and {Logical} {Rules} {Simultaneously} from {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2009.10800},
	doi = {10.48550/arXiv.2009.10800},
	abstract = {The problem of knowledge graph (KG) reasoning has been widely explored by traditional rule-based systems and more recently by knowledge graph embedding methods. While logical rules can capture deterministic behavior in a KG they are brittle and mining ones that infer facts beyond the known KG is challenging. Probabilistic embedding methods are effective in capturing global soft statistical tendencies and reasoning with them is computationally efÔ¨Åcient. While embedding representations learned from rich training data are expressive, incompleteness and sparsity in real-world KGs can impact their effectiveness. We aim to leverage the complementary properties of both methods to develop a hybrid model that learns both high-quality rules and embeddings simultaneously. Our method uses a cross feedback paradigm wherein, an embedding model is used to guide the search of a rule mining system to mine rules and infer new facts. These new facts are sampled and further used to reÔ¨Åne the embedding model. Experiments on multiple benchmark datasets show the effectiveness of our method over other competitive standalone and hybrid baselines. We also show its efÔ¨Åcacy in a sparse KG setting and Ô¨Ånally explore the connection with negative sampling.},
	language = {en},
	urldate = {2025-07-09},
	publisher = {arXiv},
	author = {Suresh, Susheel and Neville, Jennifer},
	month = sep,
	year = {2020},
	note = {arXiv:2009.10800 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tanon_completeness-aware_nodate,
	title = {Completeness-aware {Rule} {Learning} from {Knowledge} {Graphs}},
	abstract = {Knowledge graphs (KGs) are huge collections of primarily encyclopedic facts that are widely used in entity recognition, structured search, question answering, and other tasks. Rule mining is commonly applied to discover patterns in KGs. However, unlike in traditional association rule mining, KGs provide a setting with a high degree of incompleteness, which may result in the wrong estimation of the quality of mined rules, leading to erroneous beliefs such as all artists have won an award. In this paper we propose to use (in-)completeness meta-information to better assess the quality of rules learned from incomplete KGs. We introduce completeness-aware scoring functions for relational association rules. Experimental evaluation both on real and synthetic datasets shows that the proposed rule ranking approaches have remarkably higher accuracy than the state-of-the-art methods in uncovering missing facts.},
	language = {en},
	author = {Tanon, Thomas Pellissier and Stepanova, Daria and Razniewski, Simon and Mirza, Paramita and Weikum, Gerhard},
}

@inproceedings{zhu_closer_2023,
	address = {New York, NY, USA},
	series = {{IJCKG} '22},
	title = {A {Closer} {Look} at {Probability} {Calibration} of {Knowledge} {Graph} {Embedding}},
	isbn = {978-1-4503-9987-6},
	url = {https://dl.acm.org/doi/10.1145/3579051.3579072},
	doi = {10.1145/3579051.3579072},
	abstract = {When the estimated probabilities do not match the relative frequencies, we say these estimated probabilities are uncalibrated [39], which may cause incorrect decision making, and is particularly undesired in high-stakes tasks [45]. Knowledge Graph embedding models are reported to produce uncalibrated probabilities [36], e.g., for all the triples predicted with probability 0.9, the percentage of them being truly correct triples is not . In this article, we take a closer look at this problem. First, we confirmed the issue that typical KG Embedding models are uncalibrated. Then, we show how off-the-shelf calibration techniques can be used to mitigate this issue, among which binning-based calibration produces more calibrated probabilities. We also investigated the possible reasons for the uncalibrated probabilities and found that the expit transform, the way used to convert embedding scores into probabilities, is ineffective in most cases.},
	urldate = {2025-06-16},
	booktitle = {Proceedings of the 11th {International} {Joint} {Conference} on {Knowledge} {Graphs}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Ruiqi and Wang, Fangrong and Bundy, Alan and Li, Xue and Nuamah, Kwabena and Xu, Lei and Mauceri, Stefano and Pan, Jeff Z.},
	month = feb,
	year = {2023},
	pages = {104--109},
}

@inproceedings{rao_using_2024,
	address = {Singapore Singapore},
	title = {Using {Model} {Calibration} to {Evaluate} {Link} {Prediction} in {Knowledge} {Graphs}},
	isbn = {979-8-4007-0171-9},
	url = {https://dl.acm.org/doi/10.1145/3589334.3645506},
	doi = {10.1145/3589334.3645506},
	abstract = {Link prediction models assign scores to predict new, plausible edges to complete knowledge graphs. In link prediction evaluation, the score of an existing edge (positive) is ranked w.r.t. the scores of its synthetically corrupted counterparts (negatives). An accurate model ranks positives higher than negatives, assuming ascending order. Since the number of negatives are typically large for a single positive, link prediction evaluation is computationally expensive. As far as we know, only one approach has proposed to replace rank aggregations by a distance between sample positives and negatives. Unfortunately, the distance does not consider individual ranks, so edges in isolation cannot be assessed. In this paper, we propose an alternative protocol based on posterior probabilities of positives rather than ranks. A calibration function assigns posterior probabilities to edges that measure their plausibility. We propose to assess our alternative protocol in various ways, including whether expected semantics are captured when using different strategies to synthetically generate negatives. Our experiments show that posterior probabilities and ranks are highly correlated. Also, the time reduction of our alternative protocol is quite significant: more than 77\% compared to rank-based evaluation. We conclude that link prediction evaluation based on posterior probabilities is viable and significantly reduces computational costs.},
	language = {en},
	urldate = {2025-06-04},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {ACM},
	author = {Rao, Aishwarya and Krishnan, Narayanan Asuri and Rivero, Carlos R.},
	month = may,
	year = {2024},
	pages = {2042--2051},
}

@article{meilicke_anytime_2024,
	title = {Anytime bottom-up rule learning for large-scale knowledge graph completion},
	volume = {33},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-023-00800-5},
	doi = {10.1007/s00778-023-00800-5},
	abstract = {Knowledge graph completion is the task of predicting correct facts that can be expressed by the vocabulary of a given knowledge graph, which are not explicitly stated in that graph. Broadly, there are two main approaches for solving the knowledge graph completion problem. Sub-symbolic approaches embed the nodes and/or edges of a given graph into a low-dimensional vector space and use a scoring function to determine the plausibility of a given fact. Symbolic approaches learn a model that remains within the primary representation of the given knowledge graph. Rule-based approaches are well-known examples. One such approach is AnyBURL. It works by sampling random paths, which are generalized into Horn rules. Previously published results show that the prediction quality of AnyBURL is close to current state of the art with the additional benefit of offering an explanation for a predicted fact. In this paper, we propose several improvements and extensions of AnyBURL. In particular, we focus on AnyBURL‚Äôs capability to be successfully applied to large and very large datasets. Overall, we propose four separate extensions: (i) We add to each rule a set of pairwise inequality constraints which enforces that different variables cannot be grounded by the same entities, which results into more appropriate confidence estimations. (ii) We introduce reinforcement learning to guide path sampling in order to use available computational resources more efficiently. (iii) We propose an efficient sampling strategy to approximate the confidence of a rule instead of computing its exact value. (iv) We develop a new multithreaded AnyBURL, which incorporates all previously mentioned modifications. In an experimental study, we show that our approach outperforms both symbolic and sub-symbolic approaches in large-scale knowledge graph completion. It has a higher prediction quality and requires significantly less time and computational resources.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {The VLDB Journal},
	author = {Meilicke, Christian and Chekol, Melisachew Wudage and Betz, Patrick and Fink, Manuel and Stuckeschmidt, Heiner},
	month = jan,
	year = {2024},
	keywords = {Knowledge graph completion, Link prediction, Rule learning},
	pages = {131--161},
}

@article{friedman_symbolic_nodate,
	title = {Symbolic {Querying} of {Vector} {Spaces}: {Probabilistic} {Databases} {Meets} {Relational} {Embeddings}},
	abstract = {We propose unifying techniques from probabilistic databases and relational embedding models with the goal of performing complex queries on incomplete and uncertain data. We formalize a probabilistic database model with respect to which all queries are done. This allows us to leverage the rich literature of theory and algorithms from probabilistic databases for solving problems. While this formalization can be used with any relational embedding model, the lack of a well-deÔ¨Åned joint probability distribution causes simple query problems to become provably hard. With this in mind, we introduce TRACTOR, a relational embedding model designed to be a tractable probabilistic database, by exploiting typical embedding assumptions within the probabilistic framework. Using a principled, efÔ¨Åcient inference algorithm that can be derived from its deÔ¨Ånition, we empirically demonstrate that TRACTOR is an effective and general model for these querying tasks.},
	language = {en},
	author = {Friedman, Tal},
}

@article{loconte_how_nodate,
	title = {How to {Turn} {Your} {Knowledge} {Graph} {Embeddings} into {Generative} {Models}},
	abstract = {Some of the most successful knowledge graph embedding (KGE) models for link prediction ‚Äì CP, RESCAL, TUCKER, COMPLEX ‚Äì can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits ‚Äì constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.},
	language = {en},
	author = {Loconte, Lorenzo and Mauro, Nicola Di},
}

@misc{tabacof_probability_2020,
	title = {Probability {Calibration} for {Knowledge} {Graph} {Embedding} {Models}},
	url = {http://arxiv.org/abs/1912.10000},
	doi = {10.48550/arXiv.1912.10000},
	abstract = {Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get signiÔ¨Åcantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to deÔ¨Åne relation-speciÔ¨Åc decision thresholds.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Tabacof, Pedro and Costabello, Luca},
	month = feb,
	year = {2020},
	note = {arXiv:1912.10000 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yang_embedding_2015,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	doi = {10.48550/arXiv.1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	urldate = {2025-05-27},
	publisher = {arXiv},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = aug,
	year = {2015},
	note = {arXiv:1412.6575 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{islam_negative_2022,
	title = {Negative sampling and rule mining for explainable link prediction in knowledge graphs},
	volume = {250},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122005342},
	doi = {10.1016/j.knosys.2022.109083},
	abstract = {Several KG embedding methods were proposed to learn low dimensional vector representations of entities and relations of a KG. Such representations facilitate the link prediction task, in the service of inference and KG completion. In this context, it is important to achieve both an efficient KG embedding and explainable predictions. During learning of efficient embeddings, sampling negative triples was highlighted as an important step as KGs only have observed positive triples. We propose an efficient simple negative sampling (SNS) method based on the assumption that the entities which are closer in the embedding space to the corrupted entity are able to provide high-quality negative triples. As for explainability, it actually constitutes a thriving research question especially when it comes to analyze KGs with their rich semantics rooted in description logics. Hence, we propose in this paper a new rule mining method on the basis of learned embeddings. We extensively evaluate our proposals through several experiments. We evaluate our SNS sampling method plugged to several KG embedding models through link prediction task performances on well-known datasets. Experimental results show that the SNS improves the prediction performance of KG embedding models, and outperforms the existing sampling methods. To assess the performance of our rule mining method with and without SNS, we mine and evaluate rules on three popular datasets. The extracted rules are evaluated as knowledge nuggets extracted from the KG and also as support for explainable link prediction. The overall results are good and open the way to many improvements and new perspectives.},
	urldate = {2025-05-27},
	journal = {Knowledge-Based Systems},
	author = {Islam, Md Kamrul and Aridhi, Sabeur and Smail-Tabbone, Malika},
	month = aug,
	year = {2022},
	keywords = {Explainability, Knowledge graph embedding, Link prediction, Negative sampling, Rule mining},
	pages = {109083},
}

@inproceedings{zhang_iteratively_2019,
	address = {San Francisco CA USA},
	title = {Iteratively {Learning} {Embeddings} and {Rules} for {Knowledge} {Graph} {Reasoning}},
	isbn = {978-1-4503-6674-8},
	url = {https://dl.acm.org/doi/10.1145/3308558.3313612},
	doi = {10.1145/3308558.3313612},
	abstract = {Reasoning is essential for the development of large knowledge graphs, especially for completion, which aims to infer new triples based on existing ones. Both rules and embeddings can be used for knowledge graph reasoning and they have their own advantages and difficulties. Rule-based reasoning is accurate and explainable but rule learning with searching over the graph always suffers from efficiency due to huge search space. Embedding-based reasoning is more scalable and efficient as the reasoning is conducted via computation between embeddings, but it has difficulty learning good representations for sparse entities because a good embedding relies heavily on data richness. Based on this observation, in this paper we explore how embedding and rule learning can be combined together and complement each other‚Äôs difficulties with their advantages. We propose a novel framework IterE iteratively learning embeddings and rules, in which rules are learned from embeddings with proper pruning strategy and embeddings are learned from existing triples and new triples inferred by rules. Evaluations on embedding qualities of IterE show that rules help improve the quality of sparse entity embeddings and their link prediction results. We also evaluate the efficiency of rule learning and quality of rules from IterE compared with AMIE+, showing that IterE is capable of generating high quality rules more efficiently. Experiments show that iteratively learning embeddings and rules benefit each other during learning and prediction.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {ACM},
	author = {Zhang, Wen and Paudel, Bibek and Wang, Liang and Chen, Jiaoyan and Zhu, Hai and Zhang, Wei and Bernstein, Abraham and Chen, Huajun},
	month = may,
	year = {2019},
	pages = {2366--2377},
}

@article{omran_embedding-based_2021,
	title = {An {Embedding}-{Based} {Approach} to {Rule} {Learning} in {Knowledge} {Graphs}},
	volume = {33},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/8839576/},
	doi = {10.1109/TKDE.2019.2941685},
	abstract = {It is natural and effective to use rules for representing explicit knowledge in knowledge graphs. However, it is challenging to learn rules automatically from very large knowledge graphs such as Freebase and YAGO. This paper presents a new approach, RLvLR (Rule Learning via Learning Representations), to learning rules from large knowledge graphs by using the technique of embedding in representation learning together with a new sampling method. Based on RLvLR, a new method RLvLR-Stream is developed for learning rules from streams of knowledge graphs. Both RLvLR and RLvLR-Stream have been implemented and experiments conducted to validate the proposed methods regarding the tasks of rule learning and link prediction. Experimental results show that our systems are able to handle the task of rule learning from large knowledge graphs with high accuracy and outperform some state-of-the-art systems. Specifically, for massive knowledge graphs with hundreds of predicates and over 10M facts, RLvLR is much faster and can learn much more quality rules than major systems for rule learning in knowledge graphs such as AMIE+. In the setting of knowledge graph streams, RLvLR-Stream significantly improved RLvLR for both rule learning and link prediction.},
	number = {4},
	urldate = {2025-05-27},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Omran, Pouya Ghiasnezhad and Wang, Kewen and Wang, Zhe},
	month = apr,
	year = {2021},
	keywords = {Knowledge based systems, Knowledge engineering, Predictive models, Rule learning, Sampling methods, Scalability, Standards, Task analysis, knowledge graphs},
	pages = {1348--1359},
}

@incollection{damato_rule_2018,
	address = {Cham},
	title = {Rule {Induction} and {Reasoning} over {Knowledge} {Graphs}},
	volume = {11078},
	isbn = {978-3-030-00337-1 978-3-030-00338-8},
	url = {https://link.springer.com/10.1007/978-3-030-00338-8_6},
	abstract = {Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. Learning rules from KGs is a crucial task for KG completion, cleaning and curation. This tutorial presents state-ofthe-art rule induction methods, recent advances, research opportunities as well as open challenges along this avenue. We put a particular emphasis on the problems of learning exception-enriched rules from highly biased and incomplete data. Finally, we discuss possible extensions of classical rule induction techniques to account for unstructured resources (e.g., text) along with the structured ones.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {Reasoning {Web}. {Learning}, {Uncertainty}, {Streaming}, and {Scalability}},
	publisher = {Springer International Publishing},
	author = {Stepanova, Daria and Gad-Elrab, Mohamed H. and Ho, Vinh Thinh},
	editor = {d‚ÄôAmato, Claudia and Theobald, Martin},
	year = {2018},
	doi = {10.1007/978-3-030-00338-8_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {142--172},
}

@incollection{vrandecic_rule_2018,
	address = {Cham},
	title = {Rule {Learning} from {Knowledge} {Graphs} {Guided} by {Embedding} {Models}},
	volume = {11136},
	isbn = {978-3-030-00670-9 978-3-030-00671-6},
	url = {https://link.springer.com/10.1007/978-3-030-00671-6_5},
	abstract = {Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as conÔ¨Ådence reÔ¨Çect rule quality well when the KG is reasonably complete; however, these measures might be misleading otherwise. So it is difÔ¨Åcult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules is generated. Therefore, the ranking and pruning of candidate rules is a major problem. To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.},
	language = {en},
	urldate = {2025-05-27},
	booktitle = {The {Semantic} {Web} ‚Äì {ISWC} 2018},
	publisher = {Springer International Publishing},
	author = {Ho, Vinh Thinh and Stepanova, Daria and Gad-Elrab, Mohamed H. and Kharlamov, Evgeny and Weikum, Gerhard},
	editor = {Vrandeƒçiƒá, Denny and Bontcheva, Kalina and Su√°rez-Figueroa, Mari Carmen and Presutti, Valentina and Celino, Irene and Sabou, Marta and Kaffee, Lucie-Aim√©e and Simperl, Elena},
	year = {2018},
	doi = {10.1007/978-3-030-00671-6_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {72--90},
}

@article{gad-elrab_explainable_nodate,
	title = {Explainable {Methods} for {Knowledge} {Graph} {ReÔ¨Ånement} and {Exploration} via {Symbolic} {Reasoning}},
	language = {en},
	author = {Gad-Elrab, Mohamed H},
}

@inproceedings{gad-elrab_exception-enriched_2016,
	address = {Cham},
	title = {Exception-{Enriched} {Rule} {Learning} from {Knowledge} {Graphs}},
	isbn = {978-3-319-46523-4},
	doi = {10.1007/978-3-319-46523-4_15},
	abstract = {Advances in information extraction have enabled the automatic construction of large knowledge graphs (KGs) like DBpedia, Freebase, YAGO and Wikidata. These KGs are inevitably bound to be incomplete. To fill in the gaps, data correlations in the KG can be analyzed to infer Horn rules and to predict new facts. However, Horn rules do not take into account possible exceptions, so that predicting facts via such rules introduces errors. To overcome this problem, we present a method for effective revision of learned Horn rules by adding exceptions (i.e., negated atoms) into their bodies. This way errors are largely reduced. We apply our method to discover rules with exceptions from real-world KGs. Our experimental results demonstrate the effectiveness of the developed method and the improvements in accuracy for KG completion by rule-based fact prediction.},
	language = {en},
	booktitle = {The {Semantic} {Web} ‚Äì {ISWC} 2016},
	publisher = {Springer International Publishing},
	author = {Gad-Elrab, Mohamed H. and Stepanova, Daria and Urbani, Jacopo and Weikum, Gerhard},
	editor = {Groth, Paul and Simperl, Elena and Gray, Alasdair and Sabou, Marta and Kr√∂tzsch, Markus and Lecue, Freddy and Fl√∂ck, Fabian and Gil, Yolanda},
	year = {2016},
	keywords = {Horn Rules, Knowledge Graph (KG), Nonmonotonic Rule, Wikidata, Witness Set},
	pages = {234--251},
}
