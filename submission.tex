% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{hyperref}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}
%
\begin{document}
%
\title{Extending Mined Rules for Link Prediction with Schema-based Exceptions}
%
% \titlerunning{Extending Mined Rules with schema-based Exceptions}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{}
%
\authorrunning{}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Link prediction (LP) is the task of expanding information in Knowledge Graphs (KGs) by suggesting new relations among their entities. Mining and applying inference rules for this task can be used, with competitive performances, to maintain transparency with respect to machine learning methods for LP which behave as black-boxes. Yet, these rule-based methods suffer from a lack of expressivity and  rarely leverage semantic information of the KG, focusing instead on entity-level triples, leading to inconsistencies when considering the KG schema restrictions.

In this work, we study the effect of extending rule expressivity with non-monotonicity: leveraging KG schema information, we extend mined rules with exception cases, in order to make the KG more consistent and semantically accurate. We automatically define exception cases based on the predicted relations, pruning the search space when when the rule could produce semantically incorrect facts.

We evaluate our approach by analyzing the quality of the materialized triples and the additional computing time required, and we further verified the change in rank-based LP metrics, starting from rule sets mined by two rule miners.

Our results suggest that the semantic quality of mined rules varies depending on the level of curation and schema-complexity of the KG. Further, we show that applying exceptions at single-rule level (thus ignoring between-rule interactions) allows to substantially reduce the number of inconsistent triples in all cases, at the cost of a limited increase in inference time. We finally report that rank-based LP metrics are influenced by the different rule expressivity in a limited way, further suggesting the need for the integration of alternative metrics that can take into account the semantic quality of predicted triples.
% In this work, we studied the effect of extending rule expressivity with non-monotonicity: leveraging schema information, we automatically define exception cases based on the predicted relation, potentially stopping the rule from triggering and producing semantically incorrect facts. We showed that rules mined both from curated and automatically generated Knowledge Graphs might result in rules with inconsistent predictions with the latter being more prominent; and that applying exceptions at single-rule level (thus ignoring between-rule interactions) allows to substantially reduce the number of inconsistent triples in both cases, at the cost of a limited increase in inference time. 

% We additionally show that rank-based LP metrics are influenced by the different rule expressivity in a limited way, further suggesting the need for the integration of alternative metrics that can take into account the semantic quality of predicted triples.

%%%%%%%%%

% Link prediction (LP) is the task of expanding information in Knowledge Graphs (KGs) by suggesting new relations among their entities. Recent studies have explored the benefits of mining and applying inference rules for this task, as they have the advantage of transparency compared to black-box machine learning methods, while maintaining competitive performances. Nevertheless, it has been noted how the commonly used Horn Rules suffer from a lack of expressivity, and that rule miners rarely leverage semantic information of KGs, even if originally available, focusing instead on entity-level triples.
% Thus, newly predicted triples are exposed to the risk of being semantically inconsistent. Standard, rank-based, metrics 

\keywords{Knowledge Graphs  \and Link Prediction \and Rule mining \and Non-monotonic reasoning.}
\end{abstract}
%
%
%
\section{Introduction}
In recent years, Knowledge Graphs (KGs) \cite{hogan_knowledge_2022} have attracted interest due to  being inherently readable both from humans and machines, allowing both to encode expert knowledge and being easily processed algorithmically. Their usage spans from industrial applications \cite{deng_research_2022,han_construction_2022,tang_exploring_2023} to shareable scientific repositories \cite{dessi_cs-kg_2025,himmelstein_systematic_nodate,jaradeh_open_2019}, and, with the recent rise of Large Language Models, they have been applied as means to ground the generative process with reliable facts \cite{procko_graph_2024}. Despite this, KGs are often incomplete, meaning that they do not (and often cannot) capture all the facts relevant to their domain fields: they can miss information about individuals, or relations that connect them. KG incompleteness stems not only from imperfections in data extraction and expert curation but also from the fact that new knowledge is discovered. The task of KG Completion aims at addressing such incompleteness. A specific approach to KG completion is Link Prediction (LP), which aims at predicting missing links (edges) that are likely to be true in the context of the KG.

Among the techniques used for LP, Rule Mining (RM) \cite{harth_fast_2020,meilicke_anytime_2024} has found renewed interest for being human-readable compared to subsymbolic methods like KG embeddings \cite{wang_knowledge_2017} and Graph Neural Networks \cite{ye_comprehensive_2022}, while at the same time showcasing competitive performances. Most RM methods involve finding and extracting patterns in the data, which usually take the shape of paths connecting pairs of entities, for example the rule  $\textit{childOf}(X, Y),$ $ \textit{bornIn}(Y,C) \rightarrow \textit{bornIn(X,C)}$, i.e. suggesting that a child Y is often born in the same city C as a parent X. Despite being closer to symbolic methods, RM algorithms often fail to leverage the semantic information that might come with the KG, focusing instead on frequent patterns. 

Following the previous example, given the above rule, and the query (virginiaWoolf, bornIn, ?), the rule will be applied for any available matching body, and is certain to introduce inconsistencies given that her parents were born in different cities (London and Calcutta), while including information about the functionality (i.e. having at most one value) of `bornIn' would at least highlight a possible issue. This is only partially captured by standard LP metrics, which are computed from a ranking of predictions given a test set \cite{paulheim_knowledge_2016}: the ranking of the predictions might be influenced by the number of predictions, but a standard metric will not highlight the issue with consistency (a person cannot have two birth cities). This is likely to reduce the applicability of LP solutions in the practical case: while inconsistent triples from the same rule might have a limited effect on ranking metrics, they require repairing the knowledge base.

% Some previous work suggested the potential of extending RM systems with rule exceptions, taking a non-monotonic approach \cite{gad-elrab_exception-enriched_2016,tran_towards_2017,lisi_combining_nodate}, exceptions were obtained by refining the rule sets based on the available data, but still without focus on semantics.

In this work, we tackle the problem of expressivity and consistency via a non-monotonic approach, operating under the hypothesis that integrating exceptions into rules increases granularity and preempts inconsistencies. Similar methods have been previously proposed \cite{gad-elrab_exception-enriched_2016,lisi_combining_2017,tran_towards_2017}, but they typically derive exceptions by refining rule sets based solely on data distribution, leaving the semantic layer unaddressed. Our approach is instead focused on the information present in ontological schemas of KGs: given a set of semantic restrictions and a mined rule set, our approach automatically appends relevant exception conditions to the rules. These conditions act as a safeguard during the application phase, preventing the triggering of rules that would result in trivial violations of the ontology. We then compare the semantic validity of triples predicted by the original and extended rule sets to evaluate the trade-off involved in preventing trivial rule-based violations. This comparison is conducted on four large-scale KGs representing varying domains and curation levels: YAGO4.5, a general-purpose KG derived from Wikidata with a highly curated schema based on schema.org; NELL995, an automatically generated KG with a rigid, ad-hoc schema; Hetionet, a domain-specific KG featuring a smaller, high-level ontology and CSKG2.0, collecting statemnts from millions of Computer Science papers, automatically populated using a granular, expert validated, ontology.

Our results show that the semantic accuracy of the mined rules varies substantially depending on the quality of the input KG and its curation level. Specifically, we show that applying exceptions at single-rule level (i.e. without considering between-rule interaction)  results in a decrease in the number of inconsistent triples predicted with a limited increase in computational time in all three conditions, with the highest benefit in less-curated KGs. Additionally, while we do not focus on optimizing  predictive performance, we show that standard rank-based metrics are not heavily influenced by the change in expressivity of the rule sets, revealing the need for integrating such metrics with more semantic-oriented evaluations.

\section{Related Work}

Link prediction is a widespread topic and has been tackled with multiple approaches, most notably KG embeddings (KGEs), Graph Neural Networks (GNNs) and Rule Mining. 

KGEs involve mapping the entities and relations of the KG into a lower-dimensional vector representations. The methods, e.g.,\cite{bordes_translating_2013,sun_rotate_2018,trouillon_complex_2016}, usually learn the vectors' weight by minimizing a loss over a scoring function that is intended to preserve certain properties of the inner geometric structure of the KG. 

GNNs (e.g., \cite{kipf_semi-supervised_2016,zeb_complex_2022}), similarly to KGEs, are subsymbolic approaches that learn a vectorized representation of the graph, but they are usually deeper and also embed information about node features and neighborhood via message passing. For an analysis of the described methods, we refer to \cite{arrar_comprehensive_2024}.

Rule Mining \cite{zeng_logical_2023} involves the discovery of symbolical (typically Horn-like) rules from the data, which is the focus of this work: in Section \ref{sec:exp} we generated rule sets using AnyBURL \cite{meilicke_anytime_2024} and AMIE3 \cite{harth_fast_2020}, which respectively employ path sampling (bottom-up) and systematic generation and testing (top-down) with pruning heuristics.
Other rule miners have been proposed: \cite{pirro_relatedness_2020} uses a t-box based approach to suggest rule candidates, and SAFRAN \cite{ott_safran_2021} is an extension of AnyBurl using noisy-or: the confidence of a predicted triple is an aggregation of the confidence of all triggered relevant rules, once redundant rules are removed. We currently do not use noisy-or, as it would require exploring the interplay between redundancy and exceptions, but we intend to apply this in the future. 

Non-monotonic extensions of rule-sets have also been proposed in the previous years: a series of studies \cite{gad-elrab_exception-enriched_2016,lisi_combining_2017,tran_towards_2017} explores learning exceptions via a inductive logic programming approach, while we aim at exploiting ontological information. 

Finally, using t-box or ontological knowledge to improve link prediction has been the subject of study of various neuro-symbolic approaches: \cite{jain_improving_2021} and 
\cite{hubert_treat_2024} use schema-based inconsistencies to generate negative samples for KGEs or to directly operate in the training loss, \cite{wu_learning_2022} include type information in the rule learning process, \cite{wang_schema-aware_nodate,wiharja_schema_2020} focus on \textit{schema correct} triples to iteratively learn LP methods alternating rule mining and KGEs learning.

\section{Problem Definition}
In this section, we describe the basic terminology used in the rest of the discussion and the reserach problem we aim to tackle.

\subsubsection{Terminology.}
Our input is defined as a tuple $(\mathcal{G},\mathcal{O},\mathcal{IR})$ where:
\begin{itemize}
    \item $\mathcal{G}$ is a knowledge graph: $$\mathcal{G} = (\mathcal{E}, \mathcal{R}, \mathcal{T})$$ where $\mathcal{E}$ is the (finite) set of entities (nodes), $\mathcal{R}$ is the (finite) set of relation types, and $\mathcal{T} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$ is the set of known triples (edges). From the example of the previous section, \textit{(virginiaWoolf, childOf, leslieStephen)}, \textit{(leslieStephen, bornIn, London)} are facts in the KG.
    \item $\mathcal{O}$ is a set of ontological restrictions, defined in a given Description Logic formalism. (bornIn, subClassOf, FunctionalProperty) is an example of such restriction.
    \item $\mathcal{IR}$ is a set of inference rules of the type $$\bigwedge_{i=0}^{n} (S_i, b_i, O_i) \rightarrow (X, h , Y)$$ where $X,Y \in \{S_i\}\cup{\{O_i\}}$, $b_i,h \in \mathcal{R}$ and $S_i,O_i$ are interpreted as variables to be grounded with elements of $\mathcal{E}$ such that given a variable substitution $\theta$, $(S_i, b_i, O_i)_{/\theta} \in \mathcal{G}$. 
    
    In the specific scenario we consider, the substitution requires each variable to get a distinct grounding, i.e. $S_{i/\theta} \not= S_{j/\theta}\quad\forall \quad i \not=j$ and $S_{i/\theta} \not=O_{j/\theta}\quad if\quad S_i \not=O_j$.

    IR can be also seen as \textit{Horn rules}, i.e. of the type $B \rightarrow H$, where $B$ is a conjunction of atoms and $H$ is a single atom. In this work, we consider atoms as being \textit{triple patterns} of the type $(V1,p,V2)$, where the relation is known, while subject and objects are variables.

    Following the example, $\textit{childOf(X, Y)}, \textit{bornIn(Y,C)} \rightarrow \textit{bornIn(X,C)}$ is an example of such rule, the body can be grounded by \\$\theta: (X= virginiaWoolf, Y= leslieStephen, C = London)$, and would predict the head: $(virginiaWoolf, bornIn, London)$.
\end{itemize}

% $$\bigwedge_{i=1}^{n} (S_{i}, b_i, O_i) \rightarrow (S_x, h , O_y); x,y \le n$$

%\subsection{Link Prediction}\label{sec:LP}
\subsubsection{Link Prediction.}
Link Prediction (LP) refers to the task of predicting missing links in the graph. More precisely, the goal is to infer plausible triples $\textit{(s, r, o)}$ that are not present in $\mathcal{T}$ but are likely to hold.

LP is usually tested via the \textit{silver-standard} method: a set of triples from the original graph is removed, and the LP methods are tasked to rank each removed triple with respect of its possible corruptions (random substitutions of its components), where ideally the test triple is ranked among the most likely ones. 
In the context of rule mining for link prediction, most rule-sets $\mathcal{IR}$ assign a \textit{confidence} value to each rule. So, given a test triple $\textit{(s,r,o)}$, all rules that allow to infer $\textit{(s,r,?)}$ are queried, ranking the predicted $\textit{(s,r,o')}$ by an aggregation of the confidence scores of the rules which can correctly predict each triple. The same can be done for $\textit{(?,r,o)}$ ranking predicted $\textit{(s',r,o)}$.

\subsubsection{Rule Mining.}

Rule Mining (RM) is the task of constructing a set of inference rules, typically of the Horn type (Bâ†’H), each associated with a measure of its quality.

A specific and common type is the Closed Rule (CR), which is a rule where every atom appears, often expressed as a sequence of the type: $$\bigwedge_{i=1}^{n} (A_{i-1}, b_i, A_i) \rightarrow (A_0, h , A_n)$$ 
These are considered relatively easy to mine as they involve looking for closed paths within the graph structure.

Central to mining and materializing these rules is the concept of \textit{grounding}: a grounding  is a variable substitution $\theta$ that maps a given triple pattern $\text{tp}: (V1, p, V2)$, into a syntactically correct triple, i.e. both V1 and V2 are mapped to elements of $\mathcal{E}$.
    % the triple $tp_{/\theta}: (v1, p, v2) $ resulting from applying the variable substitution, is such that $tp_{/\theta} \in \mathcal{G}$
In the following section, we refer to Body Groundings (resp. Head Grounding) as a variable substitutions $\theta$ for which all triple patterns in the body (head) respectively, are grounded.
    
\subsubsection{Semantic Consistency of a KG and Violations.}

In general, a KG is consistent if $\mathcal{G} \cup \mathcal{O} \not\models \bot$. 

Assuming a consistent KG as an input, given a set of inference rules $IR$, it might still be the case that $\mathcal{G}^{IR} \cup \mathcal{O} \not\models \bot$. We define three ways in which rules can lead into inconsistencies:

\begin{itemize}
    \item \textit{in-graph violation}: in graph violations are inconsistencies derived by data already present in the graph when a rule conclusion is materialized.
    \item \textit{in-rule violation}: we define in-rule violations as inconsistencies caused by one or more conclusions materialized from a given rule two inconsistent conclusions predicted using different instances of the same rule. 
    \item \textit{out-rule violation}: comparatively, out-rule violations are inconsistent conclusions predicted using different rules.
\end{itemize}

Examples with the functional restriction:
\begin{itemize}
    \item \textit{in-graph violation} : a given \textit{(s,p,o)} conclusion would violate information already in the graph:  $\textit{(X,parent,Y)}, \textit{(X,bornIn,C)} \rightarrow \textit{(Y, bornIn, C)} $ might derive that a person \textit{persony} is born in the same city \textit{cityp} of their parents, even if the graph already contains $\textit{(persony, bornIn, city2)}$ triple, where \textit{cityp} and \textit{city2} are distinct.
    \item \textit{in-rule violation}: the same rule for a given \textit{(s,p,?)} query might get two or more different conclusions: $\textit{(X,parent,Y)}, \textit{(X,bornIn,C)} \rightarrow \textit{(Y, bornIn, C)} $, in case of parents being born in different cities, two head triples will be derived, which will generate an inconsistency.
    \item \textit{out-rule violation}: two different rules can trigger for a given query \textit{(s,p,?)}: 
    $\textit{(X,parent,Y)}, \textit{(X,bornIn,C)} \rightarrow \textit{(Y, bornIn, C)} $ and\\ $\textit{(Y,livesIn,C)} \rightarrow \textit{(Y, bornIn , C)}$. Both rule bodies can apply, possibly resulting in the prediction of inconsistent conclusions.
\end{itemize} 

\subsubsection{Research Question.}

Given the context and definitions described above, we intend to tackle the issue of the semantically-inconsistent materialization of rule sets for LP, by integrating schema knowledge about the restrictions. As computing \textit{out-rule} violations would involve a computationally complex analysis, possibly requiring to materialize a large number of them for each candidate, in this work we focus on removing \textit{in-graph} and \textit{in-rule} violations. Our research question is as follows:

\textit{\textbf{RQ:}To what extent does computationally extending LP rule sets with schema-based exceptions that eliminate in-rule and in-graph conflicts reduce inconsistencies in predicted triples, and how does this integration affect learning performance and computational efficiency?}


\section{Non-monotonic extension of rules}
We focus on the effect of removing in-rule and in-graph violations by extending the original $\mathcal{IR}$ non-monotonically, i.e. by defining exception patterns that, if matched, will stop the application of the rule. Our algorithm, and evaluation settings, can be found at the following repository: \url{https://anonymous.4open.science/r/NMRextension-A067/README.md}.


\subsection{Approach} \label{sec:approach}
\paragraph{Schema-based exceptions} Given $\{\mathcal{G,IR,O}\}$ we generate $\mathcal{IR}^{nm}$, a non-monotonic extension of the rule set, where the exceptions are sufficient conditions for in-rule and in-graph violations. Table \ref{tab:ijp} collects the list of inconsistency patterns adapted given the set of property restrictions we focus on: functionality,
% inverse functionality, 
domain and range. 

\paragraph{Materializing with exceptions} Algorithm \ref{alg:materialize} illustrates the materialization of $\mathcal{G}^{IR^{nm}}$, for each rule \textit{$r_i$}, given input $\{\mathcal{G,IR,O}\}$:
\begin{itemize}
    \item We iterate over candidates $e \in \mathcal{E}$ as a substitution of the first variable $X$ of \textit{$r_i$}.
    \item If in-graph exceptions are violated, further search for the remaining variables is halted (6-9).
    \item A path is searched from $e$ with a recursive depth-first search (DFS) in order to find a valid $\theta$ (10). If $\exists \theta: body(r)_{/\theta} \in \mathcal{G}$ ,  $head(r)_{/\theta} \not\in \mathcal{G}$ and $\mathcal{G} \cup \mathcal{O} \not\models exception(r)_{/\theta}$, the algorithm proceeds and $head(r)_{/\theta}$ is added to $\mathcal{G}^{IR^{nm}}$(11,16).
    \item Otherwise, the specific rule activation is skipped (12-14).  
\end{itemize}

Following \cite{meilicke_anytime_2024}, we implement this strategy as a DFS approach. Similarly to them, we require different variables to have different groundings (concept knwow as `object identity'). 
%%%%%%%%%%%%%%%%%%
% Pseudocode
%%%%%%%%%%%%%%%%%%%

\newcommand{\GIRnm}{\mathcal{G}^{IR^{nm}}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Oset}{\mathcal{O}}

% \vspace{1em}
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKw{Break}{break}

\Input{Graph $\G$, Set of Rules $R$, Candidates $\E$, Ontology $\Oset$}
\Output{Materialized Graph $\GIRnm$}

\BlankLine
Initialize $\GIRnm \leftarrow \G$\;
\BlankLine

\ForEach{rule $r_i \in R$}{
    \ForEach{candidate $e \in \E$}{
        \BlankLine
        $valid \leftarrow True$\;
        $possiblePredictions \leftarrow \emptyset$\;
        \BlankLine
        
        \If{in-graph violation detected for $e$}{
            $valid \leftarrow False$\;
            \textbf{continue}\;
        }
        
        \BlankLine
        \ForEach{substitution $\theta$ found via DFS($r_i$, start=$e$)}{
            \BlankLine
            \If{$body(r_i)_{/\theta} \subseteq \G$ \textbf{and} $head(r_i)_{/\theta} \notin \G$}{
                \BlankLine
                \eIf{$\G \cup \Oset \models exception(r_i)_{/\theta}$}{
                    $valid \leftarrow False$\;
                    \Break \tcp*{Stop DFS immediately}
                }{
                    Add $head(r_i)_{/\theta}$ to $possiblePredictions$\;
                }
            }
        }
        
        \BlankLine
        \If{$valid$ \textbf{and} $possiblePredictions \neq \emptyset$}{
            $\GIRnm \leftarrow \GIRnm \cup possiblePredictions$\;
        }
    }
}
\Return{$\GIRnm$}

\caption{Materialize $\mathcal{G}^{IR^{nm}}$}
\label{alg:materialize}
\end{algorithm}
%
%%%%%%%%%%%%%%%%%%
\begin{table}[h]
    \centering
    \caption{Violations  $r: body(r) \rightarrow head(r), head(r) =(A_x, h, A_y)$ assuming a valid substitution $\theta$ for r.}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Property} & \textbf{Condition for exception} \\
        \hline
        Functional(h)  & $\theta,\theta':A_{x/\theta} =A_{x/\theta'},A_{y/\theta} \neq A_{y/\theta'}, head(r)_{\theta'} \in \mathcal{G} \lor body(r)_{\theta'} \in \mathcal{G}$ \\
        % \hline
        % Inverse Functional(h)  & $\theta,\theta':A_{x/\theta} \neq A_{x/\theta'},A_{y/\theta} = A_{y/\theta'}, head(r)_{\theta'} \in \mathcal{G} \lor body(r)_{\theta'} \in \mathcal{G}$ \\
        \hline
        Domain(h) & $\theta: type(A_{x/\theta}) \sqcap domain(h) \sqsubseteq \bot$ \\
        \hline
        Range(h) & $\theta: type(A_{y/\theta}) \sqcap range(h) \sqsubseteq \bot$  \\
        \hline
    \end{tabular}   
    
    % , in C.W.A. 
    \label{tab:ijp}
\end{table}

\paragraph{Example.}  Recalling and extension of the above example, we might have the following triples in the KG: \textit{(virginiaWoolf, childOf, leslieStephen)}, \textit{(virginiaWoolf, childOf, juliaStephen)}, \textit{(leslieStephen, bornIn, London)}, \textit{(juliaStephen, bornIn, Calcutta)}, \textit{(virginiaWoolf, almaMater, kingsCollege)}. Standard rule mining materialization would activate rule $\textit{childOf(X, Y)}, \textit{bornIn(Y,C)} \rightarrow \textit{bornIn(X,C)}$ and predict both \textit{(virginiaWoolf, bornIn, London)} and \textit{(virginiaWoolf, bornIn, Calcutta)}, which are mutually inconsistent. When the schema-based exception for functionality is included, it will trigger once the second grounding is found, stopping any further search\footnote{in this case, no further groundings would be found, but for other types of predicates it might be the case}. Candidate predictions will be rejected, as the rule is `unsafe', but a lower confidence rule might then apply, i.e. $\textit{almaMater(X, U)}, $ $\textit{locatedIn(U,C)} \rightarrow \textit{bornIn(X,C)}$, which in this case will materialize only the correct triple \textit{(virginiaWoolf, bornIn, London)}. 
\subsection{Evaluation}
% As our method can currently only work for ungrounded rules, we cannot leverage the full capabilities of the methods, and thus do not aim at strictly improving benchmark metrics, nor intend to perform a direct comparison between models. Our focus is instead on changes in the semantic quality of predicted triples between the monotonic and non-monotonic version of rule sets, the cost in computational time and the residual effect of out-rule violations. Nonetheless, for completeness, we also include results from standard rank-based LP metrics, which should be intended as `validation' against loss of performance between the rule-sets versions, and not as an objective benchmarking result.

% The current limitation of our method to ungrounded rules prevents the leveraging of the full capabilities of the existing techniques. Consequently, we are not focused on strictly improving standard benchmark metrics or conducting a direct comparative analysis between models. 

Given our research objectives, our evaluation approach is concentrated on analyzing changes in the semantic quality of predicted triples between the monotonic and non-monotonic rule sets, and evaluating the added computational time cost and the residual effect of out-rule violations. Therefore, we do not focus on improving benchmark metrics and additionally, as our method can currently only work for ungrounded rules, we cannot leverage the full expressive capabilities of state-of-the art the rule miners, rendering a performance comparison of rule-set from two different miners unfeasible.
 
For completeness, we report standard rank-based LP metrics. These results are included solely for validating against performance loss between the rule-set versions and should not be interpreted as an objective benchmarking result. 

\paragraph{Materialization evaluation.} We rank the rules in descending confidence order and incrementally materialize the predicted triples. This process continues until the cumulative number of new triples reaches target thresholds of 10\% and 30\% relative to the original graph size. Processing halts upon the completion of the specific rule application that triggers the threshold.

The full graph and the newly materialized triples are then both loaded in a triple store (GraphDB\footnote{\url{https://graphdb.ontotext.com/documentation/10.3/index.html}}) as separate named graphs. 
For each considered restriction, we then compute the number of new triples that would violate such restriction, and we further compute the total number of new triples that are inconsistent. This is done via SPARQL queries, which can be seen the linked repository\footnote{We use a RDFS-plus engine, but consider entities as mutually disjoint for functionality matter.}. We additionally report the computational time for performing the materialization in both conditions, the non-monotonic case also including the schema-parsing.

\paragraph{Ranking-based LP metrics}
LP metrics are usually \textit{ranking metrics}. Given a set of test triples, i.e. positive triples that we know are in the KG but have not been seen by the model, for each belonging test triple $t_i$ we compute its ranking relative to all possible corruption of its subject and/or object (usually excluding corruptions that would be part of the known graph, known as \textit{filtered} condition).

\begin{itemize}
    \item  Hits@k: 
    \[
    \textit{Hits@}k = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I} \left[ \textit{rank}_i \leq k \right]
    \]
    Where $\mathbb{I}[\cdot]$ equals 1 if condition is true, 0 otherwise.
    \item Mean Reciprocal Rank (MRR): 
    \[
    \textit{MRR} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{\textit{rank}_i}
    \]
     \item sem@k:
    \[\textit{Sem}@K = \frac{1}{|\textit{Test}|} \sum_{t \in \textit{Test}} \frac{1}{K} \sum_{t' \in \mathcal{S}_q^K} \textit{consistent}(t',\mathcal{G},\mathcal{O})
    \]
    Where $q$ is a test triple, $\mathcal{S}_q^K$ is the set of ranked top-K predictions for its corruption, and the \textit{consistent} returns true if a given triple is consistent wrt to the KG and related schema, false otherwise. This is an adapted version of the sem@k metric, proposed by \cite{hubert_treat_2024}, which checks the number of semantically correct triples within the first $k$ ranked predictions. Originally only defined over domain and range, we extended it to include the considered restrictions.
\end{itemize}


% \begin{itemize}
%     \item sem@k:
%     \[\textit{Sem}@K = \frac{1}{|\textit{Test}|} \sum_{t \in \textit{Test}} \frac{1}{K} \sum_{t' \in \mathcal{S}_q^K} \textit{consistent}(t',\mathcal{G},\mathcal{O})
%     \]
%     Where $q$ is a test triple, $\mathcal{S}_q^K$ is the set of ranked top-K predictions for its corruption, and the \textit{consistent} returns true if a given triple is consistent wrt to the KG and related schema, false otherwise.
% \end{itemize}


%%%%%%%%%%%
\begin{table}[]
    \centering
        \caption{Analyzed datasets}
    \begin{tabular}{l|c|c|c|c}
         name & graph size (triples) & available restrictions & train size & test size   \\
         \hline
         NELL995 & $\sim500k$& f,d,r & 114k & 14.2k \\
         YAGO4.5-10 & 3.997M & f,d,r & 3.22M & 16.2k \\
         Hetionet & 2,275M & d,r & 2.21M & 22.5k\\
         CSKG-490K& 633k & d,r & 335k & 98.5k\\
         % WD18K & & \\
         % YAGO3-10 & & \\
    \end{tabular}

    \label{tab:datasets}
\end{table}

%%%%%%%%% 10% %%%%%%%%%%

\begin{table}[h]
\centering
\caption{Materialization metrics for the datasets and rule sets, stopping after the last rule which brings the number of new triples above the threshold of 10\%. For Hetionet, not functional restriction is defined, thus no violations.}
\label{tab:mat10}
\begin{tabular}{l | c c c c c c c}
\toprule
\multicolumn{8}{c}{NELL995} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 578 & 14.9k & 989 & 184 & 321 & 1.44k & 216s \\
AnyBURL + nmr & 585 & 16.9k & 691 & 0 & 0 & 691 & 11+ 501s \\
AMIE & 11 & 14.8k & 48 & 3.03k & 7.62k & 8.20k & 4s \\
AMIE + nmr & 36 & 14.6k & 6 & 0 & 0 & 6 & 11+28s \\
\midrule
\multicolumn{8}{c}{YAGO4.5-10} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 35 & 515k & 353 & 0 & 0 & 353 & 499s \\
AnyBURL + nmr & 19 & 500k & 0 & 0 & 0 & 0 & 103+120s \\
AMIE & 4 & 501k & 49 & 0 & 0 & 49 & 18s \\
AMIE + nmr & 3 & 499k & 0 & 0 & 0 & 0 & 91+15s \\
\midrule
\multicolumn{8}{c}{Hetionet} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 2 & 565k & 0* & 0 & 0 & 0 & 146s \\
AnyBURL + nmr & 2 & 565k & 0* & 0 & 0 & 0& 0.06+ 177s \\
AMIE & 2 & 663k & 0* & 26.2k & 0 & 26.2k & 37s \\
AMIE + nmr & 2 & 637k & 0* & 0 & 0 & 0 &0.06+ 42s \\
\midrule
\multicolumn{8}{c}{CSKG-490K} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 162 & 50.7k & 0* & 0 & 6 & 6 & 155s \\
AnyBURL + nmr & 162 & 50.7k & 0* & 0 & 0 & 0& 7+196s \\
AMIE & 24 & 50.9k & 0* & 0 & 8186 & 8186 &  19s \\
AMIE + nmr & 32 & 54.0k & 0* & 0 & 0 & 0 & 8+31s\\


\bottomrule
\end{tabular}


\end{table}


%%%%%%%%%%%% 30% %%%%%%%%%%%%%%

\begin{table}[h]
\centering
\caption{Materialization metrics for the datasets and rule sets, stopping after the last rule which brings the number of new triples above the threshold of 30\%. For Hetionet, no functional restriction is defined, thus no violations.}
\label{tab:mat30}
\begin{tabular}{l | c c c c c c c}
\toprule
\multicolumn{8}{c}{NELL995} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 1047 & 42.8k & 14.5k & 706 & 591 & 15.3k & 416s \\
AnyBURL + nmr & 1334 & 43.6k & 1.99k & 0  & 0 & 1.99k & 11+ 1128s \\
AMIE & 51 & 42.7k & 89 & 11.4k & 11.9k & 17.5k & 17s \\
AMIE + nmr & 84 & 42.7k & 6 & 0 & 0 & 6& 11+ 65s \\
\midrule
\multicolumn{8}{c}{YAGO4.5-10} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 175 & 980k & 51.1k & 2.07k& 0 & 52.5k & 2340 \\
AnyBURL + nmr & 218 & 988k & 22 & 0 & 0 & 22 & 96 + 1543s \\
AMIE & 47 & 1.19M & 1.52k & 112 & 0 & 1.6k & 273s \\
AMIE + nmr & 41 & 1.28M & 8 & 0 & 0 & 8 & 113 + 273s \\
\midrule
\multicolumn{8}{c}{Hetionet} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 4 & 1.15M & 0* & 0 & 0 & 0 & 1609s \\
AnyBURL + nmr & 4 & 1.15M & 0* & 0 & 0 & 0 &0.06+2528s \\
AMIE & 3 & 920k & 0* & 26.2k & 0 & 26.2k& 73s \\
AMIE + nmr & 3 & 894k & 0* & 0 & 0 & 0& 0.06+93s \\
\midrule
\multicolumn{8}{c}{CSKG-490K} \\
\midrule
Ruleset & \# trig rules & \# triples & func  & dom & ran & tot & time \\
\midrule
AnyBURL & 390 & 149k & 0* & 0 & 47 & 47 & 395s \\
AnyBURL + nmr & 387 & 149k & 0* & 0 & 0 & 0&  7+483s\\
AMIE & 84 & 149k & 0* & 0 & 17.9k & 17.9k &  64s \\
AMIE + nmr & 121 & 150k & 0* & 0 & 0 & 0 & 7+121s\\
\bottomrule
\end{tabular}


\end{table}
%%%%%%%%%%%

\begin{table}[t]
\centering
\caption{Ranking-based LP metrics.}
\label{tab:metrics_part1}
\begin{tabular}{l | c c c c}
\toprule
\multicolumn{4}{c}{NELL995} \\
\midrule
Ruleset & hits@1 & hits@10 & mrr & sem@10 \\
\midrule
AnyBURL & 0.2172 & 0.3620 & 0.2820 & 0.980  \\
AnyBURL + nmr & 0.2155 & 0.3550 & 0.2648 & 1.0 \\
AMIE & 0.1540 & 0.2424 & 0.1857 & 0.9801  \\
AMIE + nmr & 0.1544 & 0.2390 & 0.1851 & 1.0\\
\midrule
\multicolumn{4}{c}{YAGO4.5-10} \\
\midrule
Ruleset & hits@1 & hits@10 & mrr & sem@10 \\
\midrule
AnyBURL & 0.2831 & 0.3439 & 0.305 & 0.9964 \\
AnyBURL + nmr & 0.2830 & 0.3422 & 0.3047 & 1.0 \\
AMIE & 0.2549 & 0.2924 & 0.2688 & 0.9857 \\
AMIE + nmr & 0.2547 & 0.2916 & 0.2684 & 1.0 \\
\midrule
\multicolumn{4}{c}{Hetionet} \\
\midrule
Ruleset & hits@1 & hits@10 & mrr & sem@10 \\
\midrule
AnyBURL & 0.0180 & 0.0830 & 0.0400 & 0.9990 \\
AnyBURL + nmr & 0.0169 & 0.0823 & 0.0386 & 1.0 \\
AMIE & 0.0284 & 0.0995 & 0.0506 & 0.9584 \\
AMIE + nmr & 0.0275 & 0.0989 & 0.0498 & 1.0 \\
\midrule
\multicolumn{4}{c}{CSKG-490K} \\
\midrule
Ruleset & hits@1 & hits@10 & mrr & sem@10 \\
\midrule
AnyBURL & 0.0630 & 0.1488 & 0.0913 & 0.9971 \\
AnyBURL + nmr & 0.0630 & 0.1488 & 0.0913 & 1.0 \\
AMIE & 0.0756 & 0.1404 & 0.0847 & 0.9259 \\
AMIE + nmr & 0.0602 & 0.1439 & 0.0877 & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

%%%%

\section{Experiments} \label{sec:exp}
We now proceed to illustrate and analyze the empirical experiment setup for our study.
\subsection{Experimental setup}

\paragraph{Rule miners} In principle, our method can be applied on any rule set consisting of closed path rules. To perform an empirical evaluation, we study the effects of schema-based non-monotonic extensions of rules mined using two highly performing rule miners, AnyBurl and AMIE. As we the methods mine different type of rules and with different criteria, we do not aim to directly compare the two intending instead to evaluate the effect of non-monotonic extensions on rule set of different types. We thus use the following settings:
\begin{itemize}
    \item AnyBURL: only cyclic rules, runtime 1000s, min confidence: 0.1, min support: 2, rule length: 3 body atoms.
    \item AMIE3: min PCA confidence: 0.1, min confidence: 0.1, min support: 2, rule length: 2 body atoms.
\end{itemize}

With AMIE, we limit the max length of rules at 2 for performance reasons, as it performs a full search even on large KGs like the ones we analyze. Despite the trade-off, in this way we obtain closed path rules which are not strictly cyclic, and additionally  can exploit the usage of PCA confidence offered by default by the rule miner.

\paragraph{Datasets} We extract rules from the following knowledge graphs, which are selected as they come with a defined schema:

\begin{itemize}
    \item NELL995 \cite{xiong_deeppath_2017}, a benchmark knowledge base completion dataset based on the Never-Ending Language Learning (NELL) system.
    \item YAGO4.5-10: YAGO4.5 \cite{suchanek_yago_2024} is a general-purpose knowledge graph, which combines entity data from Wikidata with a cleaned, logic-consistent taxonomy primarily derived from Schema.org. We build a link prediction-oriented version of YAGO4.5, following the same approach of its ancestor YAGO3.10 \cite{dettmers_convolutional_2018}.  We maintain only A-box triples that involve entities appearing themselves in at least 10 other similar triples. 
    \item Hetionet \cite{himmelstein_systematic_nodate} a large, heterogeneous biomedical knowledge graph that integrates data from a collection public databases to connect various entities like genes, compounds, and diseases.
    \item CSKG-490K: CSKG-2.0 \cite{dessi_cs-kg_2025} is a automatically generated Knowledge Graph collecting statements from 15 million research paper on the field of Computer Science, based on a expert-defined ontology with a high granularity of properties, and technically evaluated by experts. It provides benchmark datasets, CSKG-490K is generated by collecting all triples with support $\geq5$.
\end{itemize}

When computing standard LP metrics, we use the max-aggregation approach for matching rules. In other words, we sort the predictions in lexicographic order according to the confidence of the rules bringing such predictions. While we are aware of its practical limitations~\cite{ott_rule-based_2023,ott_safran_2021}, it allows to narrow the focus on the single rules, which better fits our objective of seeing the effect of extending them with schema-based exceptions.
%%%%%%%%
\subsection{Results}


\paragraph{Materialization results.} Tables \ref{tab:mat10} and \ref{tab:mat30} report the results for the materialization at thresholds of, respectively, 10\% and 30\% new triples. `Func', `dom' and `ran' refer to the number of inconsistent triples among the materialized ones according to, respectively functional, domain and range restrictions, `tot' is the absolute number of inconsstent new triples, and the appended `nmr' refers to the non-monotonic extensions of the rule sets.

Results suggest how even high confidence rules can cause violations, albeit rare, both for functional restrictions and domain/range restrictions, with AnyBURL rule set being more likely to cause the former, and AMIE to the latter. This is not surprising when considering that AMIE3's default operation is under partial completeness assumption, which, given a triple (s,p,o) considers as `false' all (s,p,o') facts not in the data, which is effective for functional properties. 

The application of the non-monotonic extensions allows to improve consistency in  all cases, and seems to be particularly effective for the domain/range restrictions, consistently removing them. This generally comes at the expense of processing time, both as it requires to parse the schema, and for further processing of exception conditions. Nonetheless, in some cases (YAGO4.5 both at 10\% and 30\%) we notice a reduction of rule materialization time, likely due to stopping inconsistency-prone rules with many available (partial) groundings. 
% As an example: in YAGO4.5, many rules predicting \textit{gender} are mined (Appendix \ref{mined_rules}), but in many cases a candidate subject already has a \textit{gender} relation. All the rules which would  predict further \textit{gender} information would either predict inconsistent facts or fail to find complete paths, will actually be stopped , allowing the algorithm to pass on to other rules.
For example, in YAGO4.5 many rules predicting gender are mined. However, candidate subjects often already have an existing gender relation. Any rule that would add additional gender information would therefore either generate inconsistent facts or fail to produce a complete derivation path. With the non-monotonic extensions, such rules are stopped early, allowing the algorithm to move on to more promising rules instead.

The number of inconsistent triples from the standard rule sets, relative to the newly materialized ones, varies significantly depending on the dataset. NELL has the larger ratio in both the 10\% and 30\% conditions, while YAGO4.5, Hetionet and CSKG-490K maintain a low percentage of inconsistent triples overall, although YAGO4.5 reaches a noticeable fraction in the 30\% condition. In the case of Hetionet, only 2 (resp. 4) rules trigger, which have relatively low confidence and a high number of body groundings. This limits how informative they are, as we further confirm in the following section considering the predictive performance. In the case of YAGO, while a larger number of rules are applied, the number of original inconsistencies remains contained. This suggests that the KG, which is thoroughly curated, allows to mine rules of higher semantic quality, but they can still be improved by the increase in expressivity. A similar reasoning is valid for CSKG-490K, as the KG has a very granular hierarchy of properties, with most of them having a specific single range defined (i.e. \textit{improvesTask}, \textit{improvesMetric} and \textit{improvesMethod}). Rules thus tend to be more precise and the number of original inconsistencies is limited.
In contrast, NELL, that is automatically created with a low curated  ontology, results in a more noisy rule sets. 

\paragraph{Rank-based metrics.} As shown in Table \ref{tab:metrics_part1}, the influence of the non-monotonic extension on hits@k and MRR tends to be negative but limited. We attribute this to the quality and precision of high-confidence rules, which likely dominate the results at lower $k$ values: since individual rules can generate multiple predictions for (s,p,?) or (?,p,o) queries, they have the most influence with the top ranks. Given that the sem@10 scores are still not perfect, this shows that while the majority of top-10 predictions are semantically correct, some inconsistent triples are present. Our extension appears to remove them, but worsens the other LP scores: this seems to suggest that, following standard metrics, inconsistency generating rules would be considered of higher quality despite the downsides.
% However, the standard setting shows a slight decline in sem@10, suggesting that inconsistencies persist even when deriving predictions from high-confidence rules. 

Regarding the low LP performance, particularly with respect to Hetionet with near-zero scores, we posit that it is caused by our design choices. As the  dataset is highly coarse, containing a vast number of individuals but few classes and property type, and our configuration restricts the expressivity of the rule miners (AnyBURL is limited to cyclic rules and AMIE is capped at rules of length  2), the mined rules do not seem to be able to be sufficiently discriminatory. While these constraints were necessary to ensure tractability rather than to maximize performance, future work should explore applying non-monotonic extensions to more expressive rule sets. 





\section{Conclusions}
\subsubsection{Contributions}
We analyzed the effect of schema-based exceptions in rule sets for LP, which aim to reduce the number of inconsistencies caused by materialization of standard Horn-based rules. Our method focused on exceptions avoiding in-graph and in-rule violations of domain, range and functionality restrictions, implementing them at rule-grounding level, by stopping the DFS search for a given problematic (rule, candidate entity) pair.

Our empirical experiments showed that these extensions are sufficient to reduce the number of inconsistent triples, especially when considering domain and range, the cost is an increase of computational time, as the schema must be parsed and further checks are added to the algorithm, but we note that in one case the computational time was actually improved, likely by the stopping of high-support inconsistency-generating rules that are instead skipped.

We further reported how the quality of the original rule set is actually influenced by the granularity of the schema and the level of curation of the same, with KGs respecting highly curated and granular producing more consistent rules.

Finally, we show how rank-based metrics are influenced in a limited (but negative) way by the extension, suggesting that they are insensitive to the semantic quality of the predicted triples.
\subsubsection{Limitations and Future Work}
As discussed, we limited the settings of the rule miners for the sake of tractability, clearly causing a trade-off in the actual accuracy of performance. Further studies are warranted in order to fully explore the expressivity of the algorithms: while in principle schema-based exceptions can be applied to partially-materialized and longer rules, the search may require  optimization.

Additionally, we only included a limited number of restrictions, albeit common, existing in the OWL\cite{dean_owl_2004} standard. A possible research direction would be to expand this to include more complex axioms.

Finally, our method is  dependent on the presence of a schema(ontology), which is not the common setup for link prediction. While we argue for the benefits of including semantic information in the pipeline, an alternative approach would be to automatically extract it from the data, instead of having expert curation in the loop.
% \section*{Appendix}

% \subsection*{Consistency queries}
% \subsection*{Mined rules examples}\label{mined_rules}
% We report the first two highest-confidence rules mined in each dataset: \\\textbf{body groundings; std confidence; head <= body}
% \paragraph{YAGO4.5-10 - AnyBURL}
% \begin{itemize}
%     \item 1.82k; 0.994; http://schema.org/gender(X,Y) <= http://yago-knowledge.org/resource/playsIn(X,A), http://yago-knowledge.org/resource/playsIn(B,A), http://schema.org/gender(B,Y)
%     \item 682; 0.987; http://schema.org/gender(X,Y) <= http://schema.org/spouse(A,X), http://schema.org/spouse(B,A), http://schema.org/gender(B,Y)
% \end{itemize}
% \paragraph{YAGO4.5-10 - AMIE3}
% \begin{itemize}
%     \item 1.77k; 0.963; http://schema.org/gender(a,b) <= http://schema.org/gender(e,b), http://schema.org/performer(e,a)
%     \item 14.2k; 0.960; http://schema.org/spouse(a,b) <= http://schema.org/spouse(b,a)
% \end{itemize}
% \paragraph{Hetionet - AMIE3} 
% \begin{itemize}
%     \item 121k; 0.668;	http://example.org/expresses(a,b) <= http://example.org/upregulates(a,b)
%     \item 587k; 0.473;	http://example.org/expresses(a,b) <= http://example.org/expresses(a,f), http://example.org/interacts(b,f)
% \end{itemize}
% \begin{credits}
% \subsubsection{\ackname}
% This work is supported by the Hybrid Intelligence programme (\url{https://www.hybrid-intelligence-centre.nl/}), funded by a 10-year Zwaartekracht grant from the Dutch Ministry of Education, Culture and Science (NWO).
% \end{credits}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
 \bibliography{references}
%
\
\end{document}
